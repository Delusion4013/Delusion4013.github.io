

<!DOCTYPE html>
<html lang="en,zh-CN,default" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/my-favicon-32x32.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Chenkai">
  <meta name="keywords" content="">
  
    <meta name="description" content="This post summarizes reinforcement learning from classic tabular methods to ML-based approximations and recent LLM applications like RLHF.">
<meta property="og:type" content="article">
<meta property="og:title" content="RL00 - A glimpse of Reinforcement Learning">
<meta property="og:url" content="https://delusion4013.github.io/2025/06/24/RL00-A-glimpse-of-Reinforcement-Learning/index.html">
<meta property="og:site_name" content="Chenkai&#39;s Blog">
<meta property="og:description" content="This post summarizes reinforcement learning from classic tabular methods to ML-based approximations and recent LLM applications like RLHF.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.loli.net/2025/06/24/fpnHGZcXyb2rYsR.png">
<meta property="article:published_time" content="2025-06-24T12:42:24.000Z">
<meta property="article:modified_time" content="2025-06-26T19:27:20.684Z">
<meta property="article:author" content="Chenkai">
<meta property="article:tag" content="Data Science">
<meta property="article:tag" content="Reinforcement Learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2025/06/24/fpnHGZcXyb2rYsR.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>RL00 - A glimpse of Reinforcement Learning - Chenkai&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"delusion4013.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Connecting Dots</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/post-default.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="RL00 - A glimpse of Reinforcement Learning"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-06-24 08:42" pubdate>
          June 24, 2025 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.6k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          8 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">RL00 - A glimpse of Reinforcement Learning</h1>
            
            
              <div class="markdown-body">
                
                <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>This post summarizes reinforcement learning from classic tabular
methods to ML-based approximations and recent LLM applications like
RLHF.</p>
<span id="more"></span>
<h2 id="general-definition-and-applications">General definition and
applications</h2>
<h3 id="intuition-definition">Intuition &amp; Definition</h3>
<p>If you were asked, <em>“How did you learn to ride a bike or solve a
math problem?”</em>, your answer would likely involve
<strong>interaction</strong>. When learning to ride a bike, for
instance, you probably didn’t follow a step-by-step guide detailing
exact body movements. Instead, you engaged directly with the
environment—the bike, the road, your own balance—learning through trial
and error: falling, adjusting, and gradually figuring out what works.
This kind of learning—driven by feedback from experience—is at the heart
of reinforcement learning.</p>
<p>In more formal terms, <strong>reinforcement learning (RL)</strong> is
learning what to do - how to map situations to actions - so as to
maximize a numerical reward signal. The learner is not told which
actions to take, but instead must discover which actions yield the most
reward by trying them. This separates RL from supervised learning and
unsupervised learning. a third paradigm</p>
<p>Reinforcement Learning (RL) differs from supervised learning (SL) in
<strong>learning method</strong>: SL learns from a fixed dataset of
labeled examples, where each input has a known correct output provided
by an external supervisor. In contrast, RL learns through
<strong>interaction with the environment</strong>, receiving feedback in
the form of rewards rather than explicit labels, and improving behavior
via trial and error. RL also differs from unsupervised learning (USL) in
<strong>learning objective</strong>: USL seeks to discover hidden
patterns or structures in unlabeled data (e.g., clustering or
dimensionality reduction), whereas RL aims to <strong>optimize a policy
that maximizes cumulative rewards</strong>.</p>
<h3 id="components">Components</h3>
<p>Reinforcement learning (RL) is structured around two central
entities: the <strong>agent</strong> and the
<strong>environment</strong>, connected through a feedback loop
illustrated in the diagram <img
src="https://s2.loli.net/2025/06/24/SDBTILyEKAV5UFb.png" srcset="/img/loading.gif" lazyload
alt="basic-rl-loop" /> At each timestep <span
class="math inline">\(t\)</span>, the agent receives an observation
<span class="math inline">\(o_t\)</span> from the environment, takes an
action <span class="math inline">\(a_t\)</span>, and subsequently
receives a reward <span class="math inline">\(r_{t+1}\)</span>. The
environment, in turn, processes the action, updates its state, and emits
the next observation <span class="math inline">\(o_{t+1}\)</span> along
with the reward signal <span class="math inline">\(r_{t+1}\)</span>.</p>
<p>Beyond this basic interaction loop, an RL learning system comprises
several core components:</p>
<ul>
<li>a <strong>policy</strong> <span class="math inline">\(\pi\)</span>,
which maps states to actions and defines the agent's behavior;</li>
<li>a <strong>reward signal</strong>, which evaluates the immediate
desirability of actions taken;</li>
<li>a <strong>value function</strong>, which estimates the expected
cumulative reward and thus guides long-term decision-making;</li>
<li>optionally, a <strong>model of the environment</strong>, which
allows the agent to simulate and plan by predicting future states and
rewards.</li>
</ul>
<p>Together, these elements enable the agent to learn from experience
and improve its performance over time.</p>
<h3 id="applications">Applications</h3>
<p>Reinforcement learning shines in domains that require continuous
decision-making and adaptation. In <strong>robotics</strong>, RL trains
agents to control complex systems—such as <em>robotic arms</em> grasping
objects or <em>mobile robots</em> navigating unpredictable
terrains—through trial and error. On <strong>factory floors</strong>, it
optimizes processes like <em>automated control and supply-chain
logistics</em>, improving efficiency and resilience . In
<strong>finance</strong>, RL algorithms drive <em>algorithmic trading
and portfolio management</em> by learning strategies to buy, sell, or
hold assets, maximizing risk-adjusted returns . And in
<strong>gaming</strong>, RL systems such as Deep Q-Networks and
<em>AlphaZero</em> have mastered Go, StarCraft II, and Dota 2,
demonstrating extraordinary planning and strategic reasoning.</p>
<h2 id="from-tabular-methods-to-approximated-methods">From Tabular
methods to Approximated methods</h2>
<h3 id="tabular-solution-methods">Tabular solution Methods</h3>
<p>At first glance, "tabular methods" might conjure dull images of
exhaustive spreadsheets and tedious searches. So why are we still
intrigued by them? Simple: they distill reinforcement learning down to
its purest form, clearly revealing the fundamental mechanics behind RL
algorithms.</p>
<p>What are the scope of tabular methods then?</p>
<p>We start with a basic model - <strong>Multi-armed bandits</strong>
model as a representation of single state situation where <strong>value
functions</strong> are introduced, along with the classical exploration
vs. exploitation challenge.</p>
<p>Then we move on to more general problem formulation - (finite)
<strong>Markov Decision Process</strong> (MDP), introducing multiple
states into the problem formation. Now, our learning becomes sequential
and goal-oriented, reflecting real-world decision-making as we interact
repeatedly with our environment.</p>
<p>To solve MDP problem we introduced <strong>3 new
methods</strong>:</p>
<ul>
<li><strong>Dynamic Programming</strong></li>
<li><strong>Monte Carlo(MC) Method</strong></li>
<li><strong>Temporal Difference(TD) learning</strong></li>
</ul>
<figure>
<img src="https://s2.loli.net/2025/06/24/fpnHGZcXyb2rYsR.png" srcset="/img/loading.gif" lazyload
alt="DP-vs-MC-vs-TD" />
<figcaption aria-hidden="true">DP-vs-MC-vs-TD</figcaption>
</figure>
<p>As the pros and cons of each method was discovered, variation of the
above 3 methods are proposed, like MC+TD via multi-step bootstrapping,
TD+model learning &amp; planning, which will be discussed in later
post.</p>
<h3 id="approximate-solution-method">Approximate solution method</h3>
<p>The number of <strong>distinct states grows exponentially</strong> in
most realistic tasks, so storing a separate value for each state–action
pair is impossible, therefore we introduced approximate solution method
to search for a good approximate solution with limited computational
resources.</p>
<p>The problem with large state spaces is not just the memory needed for
large tables, but the time and data needed to fill them accurately. In
many of our target tasks, almost every state encountered will never have
been seen before. This propose challenges to the policy's
<strong>generalization capability</strong>. As key RL components are all
functions:</p>
<ul>
<li><em>State update function</em>s map observations to states</li>
<li><em>Value function</em>s map states to values</li>
<li><em>Policies</em> map states to actions</li>
<li><em>Models</em> map states and actions to next states and rewards.
to achieve generalization, we can reuse the tool - <strong>function
approximation</strong> - studied in supervised learning.</li>
</ul>
<p>There are a bunch of function approximator can be used:</p>
<ul>
<li>state aggregation (feature engineering) - represent state by a
feature vector</li>
<li>linear function (regression, nearest-neighbors, etc)
<ul>
<li>tabular function (look-up tables) are special case where <span
class="math inline">\(n\)</span> = number of states and where <span
class="math inline">\(w_i\)</span> is the value of each state <span
class="math inline">\(s_i\)</span>. <span class="math display">\[
v_{\mathbf w}(s)=
\begin{bmatrix}
w_1\\
\vdots\\
w_n
\end{bmatrix}^{\top}
\begin{bmatrix}
\mathbf 1\!\left(s=s_1\right)\\
\vdots\\
\mathbf 1\!\left(s=s_n\right)
\end{bmatrix}
\]</span></li>
</ul></li>
<li>non-linear function (neural network, decision tree, etc)</li>
</ul>
<p>As for the general methodology, we can <strong>approximate value
function</strong>:</p>
<ul>
<li><em>On-policy semi-gradient TD</em> &amp; <em>SARSA</em></li>
<li><em>Deep Q-Network (DQN)</em></li>
<li>Experience-replay variants</li>
</ul>
<p>or perform <strong>direct policy approximate</strong>:</p>
<ul>
<li><em>REINFORCE (Monte-Carlo Policy Gradient)</em> &amp; other
policy-gradient methods</li>
<li><em>Trust-Region Policy Optimisation (TRPO)</em> series
<ul>
<li>TRPO enforces a KL-divergence trust region via conjugate-gradient
solving.</li>
<li><em>Proximal Policy Optimisation (PPO)</em> – replaces TRPO’s
constraint with a simple clip or penalty surrogate; the current default
in many toolkits.</li>
</ul></li>
<li><em>Deterministic Policy Gradient (DPG) &amp; Deep DPG (DDPG)</em> –
actor outputs a deterministic action; critic supplies its gradient,
enabling continuous-action control.</li>
<li><em>Advantage Actor–Critic (A2C/A3C)</em> – many lightweight actors
share a global critic (or run asynchronously) to parallelise on-policy
learning.</li>
</ul>
<h2 id="rl-nowadays">RL nowadays</h2>
<h3 id="rl-with-deep-learning">RL with Deep Learning</h3>
<p>Deep learning has super-charged reinforcement learning by providing
expressive neural representations for policies, value functions, and
environment models. <strong>Trajectory Transformer</strong> (Janner et
al., <em>NeurIPS 2021</em>) shows how discretising continuous
trajectories into tokens effectively quantises the state-action space,
letting a plain Transformer do offline planning via sequence modelling,
also brought the influential Transformer work into RL context.</p>
<p>Meanwhile, <strong>Dyna-style architectures</strong> integrate
model-free Q-learning with a learned neural world-model whose simulated
roll-outs supply extra training data, blending model-based and
model-free updates in one deep RL loop.</p>
<p>At the high end, <strong>AlphaGo</strong> and its successor AlphaZero
couple deep convolutional policy/value nets with Monte-Carlo tree search
and self-play, achieving super-human performance in Go, chess, and
shogi—powerfully illustrating how CNN features and scalable RL jointly
push decision-making beyond human expertise.</p>
<h3 id="rl-with-large-language-models">RL with Large Language
Models</h3>
<p>Reinforcement Learning (RL), once a niche playground for gamers and
simulation enthusiasts aiming for super-human performance in video
games, has burst into the mainstream by joining forces with Large
Language Models (LLMs). Techniques such as RLHF, RLAIF, and RLVR have
transformed RL from a specialized skillset into a crucial step toward
training the next generation of AI—accelerating progress in the pursuit
of AGI. As RL insights merge seamlessly with LLMs, a dynamic new era in
artificial intelligence is unfolding, where machines not only learn but
actively align with human values, preferences, and intentions.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Notes/" class="category-chain-item">Notes</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Data-Science/">#Data Science</a>
      
        <a href="/tags/Reinforcement-Learning/">#Reinforcement Learning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>RL00 - A glimpse of Reinforcement Learning</div>
      <div>https://delusion4013.github.io/2025/06/24/RL00-A-glimpse-of-Reinforcement-Learning/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Chenkai</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>June 24, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/07/05/Complete-walk-through-of-confidence-intervals/" title="Complete walk through of confidence intervals">
                        <span class="hidden-mobile">Complete walk through of confidence intervals</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://delusion4013.github.io/2025/06/24/RL00-A-glimpse-of-Reinforcement-Learning/';
          this.page.identifier = '/2025/06/24/RL00-A-glimpse-of-Reinforcement-Learning/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'my-hexo-blog-7' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        Views: 
        <span id="busuanzi_value_site_pv"></span>
        
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        Visitors: 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
