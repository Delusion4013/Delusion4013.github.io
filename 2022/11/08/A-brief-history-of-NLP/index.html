

<!DOCTYPE html>
<html lang="en,zh-CN,default" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/my-favicon-32x32.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Chenkai">
  <meta name="keywords" content="">
  
    <meta name="description" content="In this piece of note, I will give an overview of NLP&#39;s development history, focusing on how recent neural approaches revolutionise the NLP field.">
<meta property="og:type" content="article">
<meta property="og:title" content="A brief history of NLP">
<meta property="og:url" content="https://delusion4013.github.io/2022/11/08/A-brief-history-of-NLP/index.html">
<meta property="og:site_name" content="Chenkai&#39;s Blog">
<meta property="og:description" content="In this piece of note, I will give an overview of NLP&#39;s development history, focusing on how recent neural approaches revolutionise the NLP field.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-08T03:15:40.000Z">
<meta property="article:modified_time" content="2023-03-22T06:42:40.840Z">
<meta property="article:author" content="Chenkai">
<meta property="article:tag" content="Artificial Intelligence">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary_large_image">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>A brief history of NLP - Chenkai&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"delusion4013.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Connecting Dots</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/post-default.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="A brief history of NLP"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-11-08 11:15" pubdate>
          November 8, 2022 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.8k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          5 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">A brief history of NLP</h1>
            
            
              <div class="markdown-body">
                
                <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this piece of note, I will give an overview of NLP's development
history, focusing on how recent neural approaches revolutionise the NLP
field.</p>
<span id="more"></span>
<p>Natural language processing could be roughly devided into 3 stages
based on the domainance methods used.
<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>
<ul>
<li>Symbolic NLP</li>
<li>Statitical NLP</li>
<li>Neural NLP</li>
</ul>
<h3 id="symbolic-nlp-1950s---early-1990s">Symbolic NLP (1950s - early
1990s)</h3>
<p>Research related to natural language processing originated roughly in
the 1950s. In the following 40 years, limited by the size of the
<strong>corpus</strong> and the <strong>computing power</strong>, early
natural language processing mainly used <strong>rule-based
methods</strong> to deal with generic natural language phenomena through
symbolic logic knowledge summarized by experts. Such rule systems are
difficult to be applied to solve real-world problems due to the
complexity of natural language.</p>
<h3 id="statistical-nlp-1990s---2010s">Statistical NLP (1990s -
2010s)</h3>
<p>The rapid <strong>advances in computational power and storage
capacity</strong>, as well as the increasing maturity of statistical
learning approaches, have led to the large-scale application of
corpus-based statistical learning methods in the field of natural
language processing.</p>
<p>The advantages of this method includes fast training speed, little
requirement for labelled data and has good performance over simple
problems.</p>
<p>Meanwhile, there are obvious limitations to this method. Statistical
approach requires transformation of the raw natural language input into
a vector form that can be processed by the machine based on empirical
rules. This expertise-dependent, manual process of transformation is
known as feature engineering (feature extraction), which is
time-consuming and not compatible for different tasks.</p>
<h3 id="neural-nlp-present">Neural NLP (present)</h3>
<p>To cope with the disadvantages of feature engineering, representation
learning and deep neural network-style machine leanring methods became
widely applied in NLP field, proposing an end-to-end solution.
Representation learning allows the machine to automatiacally recognise
patterns from input which can be used for tasks like classifcation.</p>
<p>In 2013, Tomas Mikolov proposed <strong>word2vec</strong>
method<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>,
using a shallow neural network with large scale corpus, which uses
contextual connection of each word to embed the semantics of the tokens
into a dense vector. This output of such method is called <strong>Word
Embedding</strong>.</p>
<p>This kinds of encoding methods avoid the usage of elaborate feature
engineering, and it also breaks down the barriers between different
tasks, as representation learning transformes input into a similar and
easily tranferrable vector space.</p>
<blockquote>
<p>This trend of Representation Learning has spread to knowledge graphs
(using Graph Embedding techniques) and recommender systems (using
user/item Embedding techniques).</p>
</blockquote>
<p>Shortly after, the drawback of word2vec was discovered - the same
word has different meanings in different contexts, but the word vector
given by this encoding is unique and static. Accordingly, the model
<strong>ELMo</strong> that introduces contextual Word Embedding was
born.</p>
<p>In 2017, the Transformer model was
released<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>.
Compared to ELMo-like models, the biggest breakthrough of Transformer is
that it does not use LSTM, but instead uses an <strong>attention
mechanism</strong>. This mechanism is a function that maps a query and a
set of key-value pairs to an output. The values output by the attention
mechanism are weighted sums, where the weight of each value is
calculated by the function of the query and the corresponding key of the
value.</p>
<blockquote>
<p>Some NLP researchers believe that the attention mechanism used by
transformer is a better alternative to LSTM. They believe that the
attention mechanism handles long-range dependencies better than LSTM and
has a very promising application. transformer uses an encoder-decoder
structure in its architecture. The encoder and decoder are highly
similar in structure, but not in function. The encoder consists of N
identical encoder layers. A decoder consists of N identical decoder
layers. Both encoder and decoder layers use the attention mechanism as a
core component.</p>
</blockquote>
<p>The great success of Transformer in the field of machine translation
has attracted the interest of many NLP scientists. As deep learning
algorithms evolves, their disadvantages begin to emerge - algorithm
requires massive labelled data. As the subjective natrue of congnitive
task of NLP, and the large number of tasks and domains it deals with, it
is time-consuming and labor-intensive for acquiring high quality
annotated corpora. The large scale pre-trained langauge model precisely
compensate for this shortage, helping NLP to achieve a series of
breakthroughs. On this regard, two famous models were born:
Bidirectional Encoder Representations from Transformers
(BERT)<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> and
Generative Pre-Traing of language model
(GPT)<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>.</p>
<p>GPT consists entirely of the decoder layer of the transformer, while
BERT consists entirely of the encoder layer of the transformer. The goal
of GPT is to generate human-like text, the goal of BERT, on the other
hand, is to provide better language representations that help achieve
better results for a wide range of downstream tasks. The BERT model
reaches an advanced level on a variety of NLP tasks and have greatly
improved STOA on many tasks. Now BERT has derived a large family of
models, among which the famous ones are XL-Net, RoBERTa,
ALBERT<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>,
ELECTRA, ERNIE, BERT-wwm, DistillBERT, etc.</p>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol>
<li id="fn:1">
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Natural_language_processing#History">Natural
language processing#History -
Wikipedia</a><a href="#fnref:1" rev="footnote"> ↩︎</a>
</li>
<li id="fn:2">
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Word2vec">Word2vec -
Wikipedia</a><a href="#fnref:2" rev="footnote"> ↩︎</a>
</li>
<li id="fn:3">
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">「1706.03762」 Attention Is
All You Need (arxiv.org)</a><a href="#fnref:3" rev="footnote"> ↩︎</a>
</li>
<li id="fn:4">
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">「1810.04805」 BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding (arxiv.org)</a><a href="#fnref:4" rev="footnote"> ↩︎</a>
</li>
<li id="fn:5">
<a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving
Language Understanding by Generative
Pre-Training</a><a href="#fnref:5" rev="footnote"> ↩︎</a>
</li>
<li id="fn:6">
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.11942">「1909.11942」 ALBERT: A Lite
BERT for Self-supervised Learning of Language Representations
(arxiv.org)</a><a href="#fnref:6" rev="footnote"> ↩︎</a>
</li>
</ol>
</div>
</div>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Notes/" class="category-chain-item">Notes</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Artificial-Intelligence/">#Artificial Intelligence</a>
      
        <a href="/tags/NLP/">#NLP</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>A brief history of NLP</div>
      <div>https://delusion4013.github.io/2022/11/08/A-brief-history-of-NLP/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Chenkai</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>November 8, 2022</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/11/10/Literature-Note-Transformer/" title="Literature Note - Transformer">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Literature Note - Transformer</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/09/15/Literature%20Note%20-%20BERT/" title="Literature Note - BERT">
                        <span class="hidden-mobile">Literature Note - BERT</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://delusion4013.github.io/2022/11/08/A-brief-history-of-NLP/';
          this.page.identifier = '/2022/11/08/A-brief-history-of-NLP/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'my-hexo-blog-7' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        Views: 
        <span id="busuanzi_value_site_pv"></span>
        
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        Visitors: 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
