<!DOCTYPE html>
<html lang="en,zh-CN,default" data-theme="auto">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">

  

  <title>Literature Note - Transformer | Chenkai&#39;s Blog</title>

  <!-- Meta Description -->
  
    <meta name="description" content="In this post, I would summarize key points from classic paper 
following its initial structure.">
  

  <!-- Keywords -->
  
    <meta name="keywords" content="Artificial Intelligence, NLP, Classic Papers">
  

  <!-- Author -->
  <meta name="author" content="Chenkai">

  <!-- Canonical URL -->
  
    <link rel="canonical" href="https://delusion4013.github.io/2022/11/10/Literature-Note-Transformer/">
  

  <!-- Favicon -->
  
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  

  <!-- Theme Color -->
  <meta name="theme-color" content="#ffffff" media="(prefers-color-scheme: light)">
  <meta name="theme-color" content="#1a1a1a" media="(prefers-color-scheme: dark)">

  <!-- Open Graph -->
  
    

<meta property="og:type" content="article">
<meta property="og:title" content="Literature Note - Transformer">
<meta property="og:url" content="https://delusion4013.github.io/2022/11/10/Literature-Note-Transformer/">

  <meta property="og:description" content="In this post, I would summarize key points from classic paper 
following its initial structure.">


<meta property="og:site_name" content="Chenkai&#39;s Blog">
<meta property="og:locale" content="en,zh-CN,default">


  <meta property="article:published_time" content="2022-11-10T03:03:40.000Z">
  
    <meta property="article:modified_time" content="2023-03-22T06:49:02.590Z">
  
  <meta property="article:author" content="Chenkai">

  
    
      <meta property="article:tag" content="Artificial Intelligence">
    
      <meta property="article:tag" content="NLP">
    
      <meta property="article:tag" content="Classic Papers">
    
  

  
    
      <meta property="article:section" content="Notes">
    
  


  

  <!-- Twitter Card -->
  
    

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Literature Note - Transformer">

  <meta name="twitter:description" content="In this post, I would summarize key points from classic paper 
following its initial structure.">




  

  <!-- Schema.org -->
  
    

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Literature Note - Transformer",
  "url": "https://delusion4013.github.io/2022/11/10/Literature-Note-Transformer/",
  
  "description": "In this post, I would summarize key points from classic paper 
following its initial structure.",
  
  
  "datePublished": "2022-11-10T03:03:40.000Z",
  "dateModified": "2023-03-22T06:49:02.590Z",
  "author": {
    "@type": "Person",
    "name": "Chenkai"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Chenkai&#39;s Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "/img/my-favicon-32x32.png"
    }
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://delusion4013.github.io/2022/11/10/Literature-Note-Transformer/"
  }
  
}
</script>

  

  <!-- RSS Feed -->
  
    <link rel="alternate" type="application/rss+xml" title="Chenkai&#39;s Blog" href="/atom.xml">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/main.b54d904b.css">

  <!-- Syntax Highlighting -->
  <link rel="stylesheet" href="/css/highlight.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/custom.css">

  <!-- Prevent FOUC - Inline theme toggle script -->
  <script>
    (function() {
      const storageKey = 'papermod-theme';
      const defaultTheme = 'auto';

      function getSystemTheme() {
        return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }

      function getSavedTheme() {
        try {
          return localStorage.getItem(storageKey);
        } catch (e) {
          return null;
        }
      }

      function setTheme(theme) {
        if (theme === 'auto') {
          theme = getSystemTheme();
        }
        document.documentElement.setAttribute('data-theme', theme);
      }

      // Apply theme immediately
      const savedTheme = getSavedTheme();
      const initialTheme = savedTheme || defaultTheme;
      setTheme(initialTheme);
    })();
  </script>

  <!-- Preload fonts (optional - add your font files here) -->
  <!-- <link rel="preload" href="/fonts/your-font.woff2" as="font" type="font/woff2" crossorigin> -->
<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <header class="header">
  <nav class="nav">
    <div class="logo">
      <a href="/" accesskey="h" title="Chenkai&#39;s Blog (Alt + H)">
        
          <img src="/img/my-favicon-32x32.png" alt="Chenkai&#39;s Blog" class="logo-image">
        
        <span class="logo-text">Chenkai&#39;s Blog</span>
      </a>
    </div>

    
      <ul class="menu" id="menu">
        
          
          <li>
            <a href="/"  title="Home">
              <span>Home</span>
            </a>
          </li>
        
          
          <li>
            <a href="/archives"  title="Archives">
              <span>Archives</span>
            </a>
          </li>
        
          
          <li>
            <a href="/tags"  title="Tags">
              <span>Tags</span>
            </a>
          </li>
        
          
          <li>
            <a href="/categories"  title="Categories">
              <span>Categories</span>
            </a>
          </li>
        
          
          <li>
            <a href="/about"  title="About">
              <span>About</span>
            </a>
          </li>
        
      </ul>
    

    <div class="nav-actions">
      
        <button class="search-button" id="search-button" title="Search (Alt + /)">
          <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <circle cx="11" cy="11" r="8"></circle>
            <path d="m21 21-4.35-4.35"></path>
          </svg>
        </button>
      

      
        <button class="theme-toggle" id="themeToggle" title="Toggle theme (Alt + T)">
          <svg class="moon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg class="sun" xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      

      <button class="menu-toggle" id="menuToggle" title="Menu">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
          <line x1="3" y1="12" x2="21" y2="12"></line>
          <line x1="3" y1="6" x2="21" y2="6"></line>
          <line x1="3" y1="18" x2="21" y2="18"></line>
        </svg>
      </button>
    </div>
  </nav>
</header>


  <main class="main">
    


  <article class="post-single">
    
      <nav class="breadcrumbs" aria-label="breadcrumb">
  <ol>
    <li>
      <a href="/">
        <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
          <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
          <polyline points="9 22 9 12 15 12 15 22"></polyline>
        </svg>
        Home
      </a>
    </li>

    
      
        <li>
          <span class="separator">/</span>
          <a href="/categories/Notes/">Notes</a>
        </li>
      
    

    <li>
      <span class="separator">/</span>
      <span class="current">Literature Note - Transformer</span>
    </li>
  </ol>
</nav>

    

    

    <header class="post-header">
      <h1 class="post-title">Literature Note - Transformer</h1>
      <div class="post-meta">
  
    <span class="post-meta-item">
      <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
        <line x1="16" y1="2" x2="16" y2="6"></line>
        <line x1="8" y1="2" x2="8" y2="6"></line>
        <line x1="3" y1="10" x2="21" y2="10"></line>
      </svg>
      <time datetime="2022-11-10T03:03:40.000Z">
        2022-11-10
      </time>
    </span>
  

  

  
    
    <span class="post-meta-item">
      <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <circle cx="12" cy="12" r="10"></circle>
        <polyline points="12 6 12 12 16 14"></polyline>
      </svg>
      <span>41 min read</span>
    </span>
  

  

  
    <span class="post-meta-item">
      <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2"></path>
        <circle cx="12" cy="7" r="4"></circle>
      </svg>
      <span>Chenkai</span>
    </span>
  

  
    <span class="post-meta-item post-categories">
      <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path>
      </svg>
      
        <a href="/categories/Notes/" class="category-link">Notes</a>
      
    </span>
  
</div>

    </header>

    
      


  <aside class="toc-container"
         role="navigation"
         aria-label="Table of Contents">
    <div class="toc-header">
      <button class="toc-toggle"
              aria-label="Toggle table of contents"
              aria-expanded="false"
              aria-controls="toc-content">
        <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" aria-hidden="true">
          <line x1="8" y1="6" x2="21" y2="6"></line>
          <line x1="8" y1="12" x2="21" y2="12"></line>
          <line x1="8" y1="18" x2="21" y2="18"></line>
          <line x1="3" y1="6" x2="3.01" y2="6"></line>
          <line x1="3" y1="12" x2="3.01" y2="12"></line>
          <line x1="3" y1="18" x2="3.01" y2="18"></line>
        </svg>
        <span>Table of Contents</span>
      </button>
    </div>
    <nav class="toc-content" id="toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text">0. Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#background"><span class="toc-text">2. Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model-architecture"><span class="toc-text">3. Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder-decoder-stacks"><span class="toc-text">3.1 Encoder &amp; Decoder Stacks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention"><span class="toc-text">3.2 Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#scaled-dot-product-attention"><span class="toc-text">3.2.1 Scaled Dot-Product
Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-head-attention"><span class="toc-text">3.2.2 Multi-Head Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#applications-of-attention-in-our-model"><span class="toc-text">3.2.3 Applications of
Attention in our Model</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#position-wise-feed-forward-networks"><span class="toc-text">3.3 Position-wise
Feed-Forward Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#embeddings-and-softmax"><span class="toc-text">3.4 Embeddings and Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#positional-encoding"><span class="toc-text">3.5 Positional Encoding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#why-self-attention"><span class="toc-text">4. Why self-attention?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#training"><span class="toc-text">5. Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#results"><span class="toc-text">6. Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-text">7. Conclusion</span></a></li></ol>
    </nav>
  </aside>


    

    <div class="post-content">
      <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this post, I would summarize key points from classic paper <a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1706.03762"><Attention is all you need></a>
following its initial structure.</p>
<span id="more"></span>
<blockquote>
<p><strong>Important Links</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1706.03762">Paper link</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor">Code
link</a></li>
</ul>
</blockquote>
<blockquote>
<p><strong>Acknowledgements</strong></p>
<p>Special thanks to Mu LI, who provides a wonderful review <a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pu411o7BE/">video</a> on this
article.</p>
<p>Also thanks to Lilian, for this wonderful <a
target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">blog</a>
connecting transformer's principle &amp; applications.</p>
</blockquote>
<h2 id="abstract">0. Abstract</h2>
<ul>
<li>Paper proposes a new simple network architecture, the Transformer,
based solely on attention mechanisms</li>
<li>State-of-the-art result on <em>machine translation tasks</em></li>
<li>Advantages of transformer architecture
<ul>
<li>More parallelizable &amp; require significantly less training
time</li>
<li>Generalize well to other tasks - see BERT, GPT</li>
</ul></li>
</ul>
<blockquote>
<p><strong>Writing tips</strong></p>
<p>When stating equal contribution using <code>*</code>, it's good
practice to list briefly about each member's work done.</p>
</blockquote>
<h2 id="introduction">1. Introduction</h2>
<blockquote>
<p>Basically an extension to abstract</p>
</blockquote>
<ul>
<li><p>Recurrent neural networks /models'</p>
<ul>
<li><p><strong>Principle</strong> Generate a sequence of hidden states
<span class="math inline">\(h_t\)</span>, as a function of the previous
hidden state <span class="math inline">\(h_{t−1}\)</span> and the input
for position <span class="math inline">\(t\)</span>.</p>
<figure>
<img src="https://s2.loli.net/2022/11/24/SxYCGEHjplryNwa.png"
alt="RNN-calculation-example.png" />
<figcaption aria-hidden="true">RNN-calculation-example.png</figcaption>
</figure></li>
<li><p><strong>Drawbacks</strong></p>
<ul>
<li>Parallelization preclusion by calculation method</li>
<li>High memory requirements for preserving historical information. /
Possibility in forgetting early information after step by step
passing.</li>
</ul></li>
</ul></li>
<li><p>Attention Mechanism</p>
<ul>
<li>An integral part of sequence &amp; transduction models.</li>
<li>Allow the model to capture dependencies between in/out sequence
regardless of their distance.</li>
<li><mark>Most of them used in conjunction with a recurrent
network</mark></li>
</ul></li>
<li><p>Transformer's innovation</p>
<ul>
<li>“eschewing recurrence and instead relying entirely on an attention
mechanism”</li>
</ul></li>
</ul>
<h2 id="background">2. Background</h2>
<ul>
<li>Introduces previous attempts to reduce sequential computation -
using CNN (convolutional neural networks)
<ul>
<li>“difficult to learn dependencies between distant positions”, between
long sequences</li>
<li>Why Multi-Head Attention (instead of single head)?
<ul>
<li>Attention mechanism eschewed convolution mechanism, losing the
opportunity to model different patterns.</li>
<li>Using Multi-Head Attention mechanism is to <mark>simulate the
multi-channel output of CNN.</mark></li>
</ul></li>
</ul></li>
<li>Previous success on self-attention</li>
<li>End-to-end memory networks’ scope and performance</li>
<li>Innovation point about Transformer.</li>
</ul>
<h2 id="model-architecture">3. Model Architecture</h2>
<p>Follows most competitive sequence transduction models, Transformer
uses an <em>encoder-decoder structure</em>, where</p>
<ul>
<li>Encoder maps input sequence to a vector-like representation for
model usage.</li>
<li>Decoder generates output sequence one element at a time.</li>
</ul>
<p>The model is <strong>auto-regressive</strong>, using output from
previous moment as additional input.</p>
<h3 id="encoder-decoder-stacks">3.1 Encoder &amp; Decoder Stacks</h3>
<figure>
<img src="https://s2.loli.net/2022/11/24/vdOCy9T3uxVQWlw.png"
alt="Transformer-Architecture-from-Paper.png" />
<figcaption
aria-hidden="true">Transformer-Architecture-from-Paper.png</figcaption>
</figure>
<ul>
<li>Encoder - Decoder, shown in the left and right halves
respectively.</li>
<li>Each layer of <strong>encoder</strong> consists of two sub-layers -
a multi-head attention mechanism &amp; a MLP. The residual connection
(inspired by ResNet) is used within sub-layers. The
<strong>LayerNorm</strong> technique is also implemented.
<ul>
<li><strong>LayerNorm</strong> is a different normalization method from
Batch Normalization and is more suitable with models taking
variable-length inputs (temporal sequences), reducing the impact of
different batch cuts (sequence length variations) on normalization.</li>
<li>Generally speaking, the difference lies in the data slicing
mechanism. BN slices the data according to the batch and regularizes the
feature dimensions, while LN slices the data according to the input
samples and regularizes different features of the same sample.</li>
</ul></li>
<li>In addition to encoder, decoder adds another layer, calculate the
multi-head attention over the output of the encoder. A masking mechanism
is used to prevent decoder uses output after position <span
class="math inline">\(i\)</span> to predict the position <span
class="math inline">\(i\)</span>.</li>
</ul>
<h3 id="attention">3.2 Attention</h3>
<p>Attention is a mechanism that model learns to <mark>make predictions
by selectively attending</mark> to a given set of
data.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>
<p>Self-attention is a type of attention mechanism where the model makes
prediction for one part of a data sample using other parts of the
observation about the same sample,... it is
<mark>permutation-invariant</mark>; in other words, it is an operation
on sets.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>
<ul>
<li>In transformer's Encoder, Query, Key, Value are identical vectors
generated by the embedding layer, therefore no trainable variables
involved.</li>
</ul>
<p>“An attention function can be described as mapping a query and a set
of key-value pairs to an output. The output is computed as a
<mark>weighted sum of the values</mark>, where the weight assigned to
each value is computed by a compatibility function of the query with the
corresponding key.”</p>
<p>The amount of attention is quantified by learned weights and thus the
output is usually in the form of weighted average over input (Query,
Key, Value) pair.</p>
<h4 id="scaled-dot-product-attention">3.2.1 Scaled Dot-Product
Attention</h4>
<ul>
<li>What's Dot-Product?
<ul>
<li>Perform matrix multiplication on queries (Q) and keys (K)
embeddings.</li>
</ul></li>
<li>Why Dot-Product?
<ul>
<li>Faster &amp; space-efficient in practice</li>
</ul></li>
</ul>
<blockquote>
<p>Another alternative for attention functions are additive
attention.</p>
</blockquote>
<ul>
<li>What's Scale?
<ul>
<li>After dot-product, divide the result by <span
class="math inline">\(\sqrt{d_k}\)</span>.</li>
</ul></li>
<li>Why scale?
<ul>
<li>For long input sequence, apply <code>softmax</code> on the unscaled
dot-product could result in <strong>Vanishing gradient</strong>.</li>
</ul></li>
</ul>
<p><span class="math display">\[
Attention(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<figure>
<img src="https://s2.loli.net/2022/11/24/4EAL3FWzb69QIpH.png"
alt="Scaled Dot-Product Attention" />
<figcaption aria-hidden="true">Scaled Dot-Product Attention</figcaption>
</figure>
<h4 id="multi-head-attention">3.2.2 Multi-Head Attention</h4>
<ul>
<li><p>What is Multi-head attention?</p>
<ul>
<li><img src="https://s2.loli.net/2022/11/24/aJTNOvFq7nfwRYP.png"
alt="Multi-head attention illustration" /> <span class="math display">\[
  \begin{aligned}
  \text{MultiHead}(Q,K,V) = \text{Concat}(head_1,...,head_h)W^O \\
  where\ head_i=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)
  \end{aligned}
  \]</span></li>
</ul></li>
<li><p>Why multi-head?</p>
<ul>
<li><blockquote>
<p>Multi-head attention allows the model to jointly attend to
information from different representation subspaces at different
positions</p>
</blockquote></li>
<li>Just like CNN's different convolution kernel, multi-head attention
enables the model to capture different input patterns, learning more
representations.</li>
</ul></li>
</ul>
<h4 id="applications-of-attention-in-our-model">3.2.3 Applications of
Attention in our Model</h4>
<ol type="1">
<li>In "encoder-decoder attention" layers, the queries come from the
previous decoder layer, and the memory keys and values come from the
output of the encoder.</li>
<li>The encoder contains self-attention layers</li>
<li>The decoder contains self-attention layers.</li>
</ol>
<h3 id="position-wise-feed-forward-networks">3.3 Position-wise
Feed-Forward Networks</h3>
<p>Put it in other way - it's a fully-connected feed-forward network
with one hidden layer (MLP).</p>
<p>This layer applies a linear transformation (<span
class="math inline">\(xW_1+b_1\)</span>) on input <code>x</code>, using
a ReLU activation function and then another linear transformation.</p>
<p>One thing to notice - the first transformation expand the input
dimension to <span class="math inline">\(2048\)</span>, and then project
it back in the second transformation. <span class="math display">\[
    FFN(x) = \max(0, xW_1+b_1)W_2+b_2
\]</span></p>
<ul>
<li>What is position-wise?
<ul>
<li>the above linear transformation is applied to <mark>every
position</mark> of the output separately and identically.</li>
<li>To put it in simple ways, it is applied to each word of the input
sequence, and is therefore called <strong>point-wise</strong></li>
</ul></li>
</ul>
<blockquote>
<p>Comparison between transformer &amp; RNN</p>
<p>Transformer extracts sequence information when applying attention
function, while the RNN directly passes the <span
class="math inline">\(t-1\)</span> information to the next computing
block.</p>
</blockquote>
<h3 id="embeddings-and-softmax">3.4 Embeddings and Softmax</h3>
<ul>
<li>What's embedding?
<ul>
<li>Embedding are a vector form of input token, used to represent
semantic information.</li>
</ul></li>
</ul>
<p>In Transformer, embeddings are used for inputs of both encoder and
decoder. Same weight matrix are applied for these embeddings and
pre-softmax linear transformation.</p>
<ul>
<li>Why multiply <span class="math inline">\(\sqrt{d_{model}}\)</span>
in embedding layer?
<ul>
<li>When learning embeddings, the L2 Norm value is applied. With the
increase of input dimension, the weight learnt would decrease.</li>
<li>However, this embedding needs to combine with the positional
encoding (with increasing integer number). To have roughly same scale of
value, <span class="math inline">\(\sqrt{d_{model}}\)</span> is
multiplied.</li>
</ul></li>
</ul>
<h3 id="positional-encoding">3.5 Positional Encoding</h3>
<p>As discussed in previous section, attention mechanism itself are
operations on sets. In other words, changing the order of encoder input
would not affect its output.</p>
<p>To add sequence information, we applied "positional encoding". In
transformer, sine and cosine functions of different frequencies are
used.</p>
<p><span class="math display">\[
\begin{aligned}
PE_{(pos,2i)}=&amp;sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}=&amp;cos(pos/10000^{2i/d_{model}})\\
&amp;\text{where pos is position and i is dimension}
\end{aligned}
\]</span></p>
<h2 id="why-self-attention">4. Why self-attention?</h2>
<ul>
<li><p>Comparison between self-attention layers to the recurrent and
convolutional layers on seq2seq tasks.</p></li>
<li><p>Self attention is evaluated on three criteria (as following
column):</p>
<ol type="1">
<li><p>Computation speed</p></li>
<li><p>Amount of parallelizable computation (smaller <span
class="math inline">\(O(x)\)</span> the better)</p></li>
<li><p>Path length of signals have to traverse in the network between
long range dependencies.</p>
<blockquote>
<p>“The shorter these paths between any combination of positions in the
input and output sequences, the easier it is to learn long-range
dependencies."</p>
</blockquote></li>
</ol></li>
</ul>
<table>
<colgroup>
<col style="width: 29%" />
<col style="width: 25%" />
<col style="width: 22%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr>
<th>Layer Type</th>
<th>Complexity per Layer</th>
<th>Sequential Operation</th>
<th>Maximum Path Length</th>
</tr>
</thead>
<tbody>
<tr>
<td>Self-Attention</td>
<td><span class="math inline">\(O(n^2\cdot d)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
</tr>
<tr>
<td>Recurrent</td>
<td><span class="math inline">\(O(n\cdot d^2)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
<td><span class="math inline">\(O(n)\)</span></td>
</tr>
<tr>
<td>Convolutional</td>
<td><span class="math inline">\(O(k\cdot n\cdot d^2)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(log_k(n))\)</span></td>
</tr>
<tr>
<td>Self-Attention(restricted)</td>
<td><span class="math inline">\(O(r\cdot n\cdot d)\)</span></td>
<td><span class="math inline">\(O(1)\)</span></td>
<td><span class="math inline">\(O(n/r)\)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p><span class="math inline">\(n\)</span> - input sequence length, <span
class="math inline">\(d\)</span> - the representation dimensionality,
<span class="math inline">\(k\)</span> - kernel width, <span
class="math inline">\(r\)</span> - restricted neighborhood size</p>
</blockquote>
<ul>
<li><span class="math inline">\(n\)</span> is normally smaller than
<span class="math inline">\(d\)</span>, giving self-attention advantages
in computing.</li>
<li><span class="math inline">\(k\)</span> is usually a small number,
therefore CNN &amp; RNN has roughly same level of complexity.</li>
<li>Adding restriction to self-attention is a trade-off between
capturing long range dependencies and computation speed.</li>
</ul>
<h2 id="training">5. Training</h2>
<p>In this section, training regime is described.</p>
<ul>
<li><p>Encode setting - byte-pair encoding</p></li>
<li><p>Dataset - larger WMT 2014 English-French dataset consisting of
36M sentences</p></li>
<li><p>Batch - a set of sentence pairs with approximately 25000 source
tokens and 25000 target tokens</p></li>
<li><p>Hardware - 8 NVIDIA P100 GPUs (still affordable, compared to GPT
:P)</p>
<ul>
<li>TPU is implemented afterwards to facilitate transformer like
computation (with large matrix multiplication)</li>
</ul></li>
<li><p>Time</p>
<ul>
<li>Base model - 0.4s step time,100,000 steps / 12 hours</li>
<li>big model - 1.0s step time, 300,000 steps / 3.5 days</li>
</ul></li>
<li><p>Optimizer</p>
<ul>
<li>Adam, <span
class="math inline">\(\beta_1=0.9,\beta_2=0.98,\epsilon=10^{-9}\)</span></li>
</ul></li>
<li><p>Scheduling</p>
<ul>
<li><span class="math inline">\(lrate = d^{-0.5}_{model} \cdot
min(step\_num^{-0.5}, step\_ num \cdot
warmup\_steps^{-1.5})\)</span></li>
<li>"This corresponds to increasing the learning rate linearly for the
first <code>warmup_steps</code> training steps, and decreasing it
thereafter proportionally to the inverse square root of the step number.
We used warmup_steps = 4000."</li>
</ul></li>
<li><p>Regularization</p>
<ul>
<li>Residual dropout
<ul>
<li>output of each sub-layer, sums of the embeddings and the positional
encodings in both the encoder and decoder stacks.</li>
<li><span class="math inline">\(P_{drop}=0.1\)</span></li>
</ul></li>
<li>Label smoothing
<ul>
<li>value <span class="math inline">\(\epsilon_{ls}=0.1\)</span></li>
<li>hurts perplexity, but improves accuracy &amp; BLUE score.</li>
</ul></li>
</ul></li>
</ul>
<h2 id="results">6. Results</h2>
<p>In this section, results on machine translation (English-to-German),
different model architecture, English Constituency Parsing tasks are
summarized.</p>
<h2 id="conclusion">7. Conclusion</h2>
<ul>
<li>Transformer - "The first sequence transduction model based entirely
on attention"</li>
<li>Achieved a new state of the art on machine translation task</li>
<li>Proposing a new model architecture for handling multi modalities
&amp; adapting to different downstream tasks (see more in BERT)</li>
</ul>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol>
<li id="fn:1">
<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">The
Transformer Family | Lil'Log
(lilianweng.github.io)</a><a href="#fnref:1" rev="footnote"> ↩︎</a>
</li>
</ol>
</div>
</div>
</div>

    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
  <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
    <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path>
    <line x1="7" y1="7" x2="7.01" y2="7"></line>
  </svg>
  <span class="tags-label">Tags:</span>
  <ul class="tags-list">
    
      <li>
        <a href="/tags/Artificial-Intelligence/" class="tag-link">#Artificial Intelligence</a>
      </li>
    
      <li>
        <a href="/tags/NLP/" class="tag-link">#NLP</a>
      </li>
    
      <li>
        <a href="/tags/Classic-Papers/" class="tag-link">#Classic Papers</a>
      </li>
    
  </ul>
</div>

      

      
  

  <div class="share-buttons">
    <span class="share-label">
      <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <circle cx="18" cy="5" r="3"></circle>
        <circle cx="6" cy="12" r="3"></circle>
        <circle cx="18" cy="19" r="3"></circle>
        <line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line>
        <line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line>
      </svg>
      Share
    </span>

    <div class="share-buttons-list">
      
        
          <a href="https://twitter.com/intent/tweet?text=Literature%20Note%20-%20Transformer&url=https%3A%2F%2Fdelusion4013.github.io%2F2022%2F11%2F10%2FLiterature-Note-Transformer%2F"
             target="_blank"
             rel="noopener noreferrer"
             class="share-button share-twitter"
             title="Share on Twitter"
             aria-label="Share on Twitter">
            <svg viewBox="0 0 24 24" fill="currentColor">
              <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
            </svg>
          </a>
        
      
        
          <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdelusion4013.github.io%2F2022%2F11%2F10%2FLiterature-Note-Transformer%2F"
             target="_blank"
             rel="noopener noreferrer"
             class="share-button share-facebook"
             title="Share on Facebook"
             aria-label="Share on Facebook">
            <svg viewBox="0 0 24 24" fill="currentColor">
              <path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"/>
            </svg>
          </a>
        
      
        
          <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fdelusion4013.github.io%2F2022%2F11%2F10%2FLiterature-Note-Transformer%2F"
             target="_blank"
             rel="noopener noreferrer"
             class="share-button share-linkedin"
             title="Share on LinkedIn"
             aria-label="Share on LinkedIn">
            <svg viewBox="0 0 24 24" fill="currentColor">
              <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
            </svg>
          </a>
        
      
        
          <a href="https://reddit.com/submit?url=https%3A%2F%2Fdelusion4013.github.io%2F2022%2F11%2F10%2FLiterature-Note-Transformer%2F&title=Literature%20Note%20-%20Transformer"
             target="_blank"
             rel="noopener noreferrer"
             class="share-button share-reddit"
             title="Share on Reddit"
             aria-label="Share on Reddit">
            <svg viewBox="0 0 24 24" fill="currentColor">
              <path d="M12 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 0 1-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 0 1 .042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 0 1 4.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 0 1 .14-.197.35.35 0 0 1 .238-.042l2.906.617a1.214 1.214 0 0 1 1.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 0 0-.231.094.33.33 0 0 0 0 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 0 0 .029-.463.33.33 0 0 0-.464 0c-.547.533-1.684.73-2.512.73-.828 0-1.979-.196-2.512-.73a.326.326 0 0 0-.232-.095z"/>
            </svg>
          </a>
        
      
        
          <a href="https://api.whatsapp.com/send?text=Literature%20Note%20-%20Transformer%20https%3A%2F%2Fdelusion4013.github.io%2F2022%2F11%2F10%2FLiterature-Note-Transformer%2F"
             target="_blank"
             rel="noopener noreferrer"
             class="share-button share-whatsapp"
             title="Share on WhatsApp"
             aria-label="Share on WhatsApp">
            <svg viewBox="0 0 24 24" fill="currentColor">
              <path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198 0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479 0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87 0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86 0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64 0 5.122 1.03 6.988 2.898a9.825 9.825 0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815 0 0012.05 0C5.495 0 .16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882 0 005.683 1.448h.005c6.554 0 11.890-5.335 11.893-11.893a11.821 11.821 0 00-3.48-8.413z"/>
            </svg>
          </a>
        
      
        
          <a href="https://telegram.me/share/url?url=https%3A%2F%2Fdelusion4013.github.io%2F2022%2F11%2F10%2FLiterature-Note-Transformer%2F&text=Literature%20Note%20-%20Transformer"
             target="_blank"
             rel="noopener noreferrer"
             class="share-button share-telegram"
             title="Share on Telegram"
             aria-label="Share on Telegram">
            <svg viewBox="0 0 24 24" fill="currentColor">
              <path d="M11.944 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0a12 12 0 0 0-.056 0zm4.962 7.224c.1-.002.321.023.465.14a.506.506 0 0 1 .171.325c.016.093.036.306.02.472-.18 1.898-.962 6.502-1.36 8.627-.168.9-.499 1.201-.82 1.23-.696.065-1.225-.46-1.9-.902-1.056-.693-1.653-1.124-2.678-1.8-1.185-.78-.417-1.21.258-1.91.177-.184 3.247-2.977 3.307-3.23.007-.032.014-.15-.056-.212s-.174-.041-.249-.024c-.106.024-1.793 1.14-5.061 3.345-.48.33-.913.49-1.302.48-.428-.008-1.252-.241-1.865-.44-.752-.245-1.349-.374-1.297-.789.027-.216.325-.437.893-.663 3.498-1.524 5.83-2.529 6.998-3.014 3.332-1.386 4.025-1.627 4.476-1.635z"/>
            </svg>
          </a>
        
      

      <!-- Copy Link Button -->
      <button class="share-button share-copy" id="copy-link-button" title="Copy link" aria-label="Copy link">
        <svg class="copy-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
          <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
          <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
        </svg>
        <svg class="check-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="display: none;">
          <polyline points="20 6 9 17 4 12"></polyline>
        </svg>
      </button>
    </div>
  </div>



      
        <nav class="post-nav">
  
    <a href="/2022/12/07/Introduction-to-ZettleKasten-Managing-your-note-and-knowledge/" class="post-nav-item post-nav-prev" rel="prev">
      <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <line x1="19" y1="12" x2="5" y2="12"></line>
        <polyline points="12 19 5 12 12 5"></polyline>
      </svg>
      <div class="post-nav-content">
        <span class="post-nav-label">Previous</span>
        <span class="post-nav-title">Introduction to ZettleKasten - Your Personal Knowledge Management System</span>
      </div>
    </a>
  

  
    <a href="/2022/11/08/A-brief-history-of-NLP/" class="post-nav-item post-nav-next" rel="next">
      <div class="post-nav-content">
        <span class="post-nav-label">Next</span>
        <span class="post-nav-title">A brief history of NLP</span>
      </div>
      <svg class="icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <line x1="5" y1="12" x2="19" y2="12"></line>
        <polyline points="12 5 19 12 12 19"></polyline>
      </svg>
    </a>
  
</nav>

      

      
        <script>
// Code copy functionality is initialized in main.js
// This partial is just a marker that code copy buttons should be enabled
</script>

      
    </footer>
  </article>

  


  </main>

  <footer class="footer">
  <div class="footer-container">
    
      <div class="social-icons">
        
          <a href="https://github.com/Delusion4013" target="_blank" rel="noopener noreferrer" title="github">
            
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" width="20" height="20">
                <path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"/>
              </svg>
            
          </a>
        
          <a href="/atom.xml" target="_blank" rel="noopener noreferrer" title="rss">
            
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" width="20" height="20">
                <path d="M4 11a9 9 0 0 1 9 9"></path>
                <path d="M4 4a16 16 0 0 1 16 16"></path>
                <circle cx="5" cy="19" r="1"></circle>
              </svg>
            
          </a>
        
      </div>
    

    <div class="footer-info">
      

      <div class="copyright">
        &copy; 2020 - 2025 Chenkai
      </div>

      
        <div class="powered-by">
          
            Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>
          
          
            <span class="separator">|</span>
          
          
            Theme <a href="https://github.com/yourusername/hexo-theme-papermod" target="_blank" rel="noopener">PaperMod</a>
          
        </div>
      
    </div>
  </div>
</footer>


  
    <button class="scroll-top" id="scrollTop" title="Back to top (Alt + G)">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" width="24" height="24">
    <path d="m18 15-6-6-6 6"/>
  </svg>
</button>

  

  
    
  <div class="search-overlay" id="search-overlay">
    <div class="search-container">
      <div class="search-header">
        <div class="search-input-wrapper">
          <svg class="search-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <circle cx="11" cy="11" r="8"></circle>
            <path d="m21 21-4.35-4.35"></path>
          </svg>
          <input
            type="text"
            id="search-input"
            class="search-input"
            placeholder="Search..."
            autocomplete="off"
            spellcheck="false"
          />
          <button class="search-close" id="search-close" aria-label="Close">
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
              <line x1="18" y1="6" x2="6" y2="18"></line>
              <line x1="6" y1="6" x2="18" y2="18"></line>
            </svg>
          </button>
        </div>
        <div class="search-hint">
          <kbd>⌘</kbd><kbd>K</kbd> or <kbd>Ctrl</kbd><kbd>K</kbd> to open, <kbd>Esc</kbd> to close
        </div>
      </div>

      <div class="search-results" id="search-results"></div>
    </div>
  </div>


  

  <!-- Scripts -->
  <script src="/js/main.02905c6e.js"></script>
  <script src="/js/theme-toggle.4f90f5d1.js"></script>

  
    <script src="/js/search.31192334.js"></script>
  

  
    <script src="/js/toc-scroll.788c21d9.js"></script>
  

  
    
  <script>
    // Keyboard shortcuts (AccessKeys)
    const shortcuts = {
      'h': () => window.location.href = '/',
      't': () => {
        const themeToggle = document.getElementById('themeToggle');
        if (themeToggle) themeToggle.click();
      },
      'c': () => {
        const tocContainer = document.querySelector('.toc-container');
        if (tocContainer) {
          tocContainer.classList.toggle('toc-open');

          // Update ARIA state
          const tocToggle = tocContainer.querySelector('.toc-toggle');
          if (tocToggle) {
            const isOpen = tocContainer.classList.contains('toc-open');
            tocToggle.setAttribute('aria-expanded', isOpen ? 'true' : 'false');
          }
        }
      },
      'g': () => {
        window.scrollTo({ top: 0, behavior: 'smooth' });
      },
      '/': () => {
        const searchButton = document.getElementById('searchButton');
        if (searchButton) searchButton.click();
      },
    };

    document.addEventListener('keydown', (e) => {
      if (e.altKey && shortcuts[e.key]) {
        e.preventDefault();
        shortcuts[e.key]();
      }
    });
  </script>


  

  




  <!-- Math Rendering -->
  
  

  
    <script>
      if (!window.MathJax) {
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            tags: 'ams',
            processEscapes: true,
            processEnvironments: true
          },
          chtml: {
            scale: 1.0,
            minScale: 0.5,
            matchFontHeight: true
          },
          options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            ignoreHtmlClass: 'tex2jax_ignore',
            processHtmlClass: 'tex2jax_process'
          },
          startup: {
            pageReady: function() {
              return MathJax.startup.defaultPageReady().then(function() {
                console.log('MathJax rendering complete');
              });
            }
          }
        };
      }
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
  


</body>
</html>
