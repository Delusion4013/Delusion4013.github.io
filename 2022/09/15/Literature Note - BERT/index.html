

<!DOCTYPE html>
<html lang="en,zh-CN,default" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/my-favicon-32x32.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Chenkai">
  <meta name="keywords" content="">
  
    <meta name="description" content="In this post, I would summarize key points from classic paper &lt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&gt; following its initial structure.">
<meta property="og:type" content="article">
<meta property="og:title" content="Literature Note - BERT">
<meta property="og:url" content="https://delusion4013.github.io/2022/09/15/Literature%20Note%20-%20BERT/index.html">
<meta property="og:site_name" content="Chenkai&#39;s Blog">
<meta property="og:description" content="In this post, I would summarize key points from classic paper &lt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&gt; following its initial structure.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://miro.medium.com/max/1400/0*m_kXt3uqZH9e7H4w.png">
<meta property="article:published_time" content="2022-09-15T20:03:40.000Z">
<meta property="article:modified_time" content="2023-03-22T06:48:55.963Z">
<meta property="article:author" content="Chenkai">
<meta property="article:tag" content="Artificial Intelligence">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Classic Papers">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://miro.medium.com/max/1400/0*m_kXt3uqZH9e7H4w.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Literature Note - BERT - Chenkai&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"delusion4013.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"|","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Connecting Dots</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/post-default.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Literature Note - BERT"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-09-15 16:03" pubdate>
          September 15, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.8k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          6 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Literature Note - BERT</h1>
            
            
              <div class="markdown-body">
                
                <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this post, I would summarize key points from classic paper <a
target="_blank" rel="noopener" href="http://arxiv.org/abs/1810.04805">&lt;BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding&gt;</a> following
its initial structure.</p>
<span id="more"></span>
<blockquote>
<p>Important Links</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1810.04805">Paper</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/google-research/bert">Official
Code</a></li>
</ul>
</blockquote>
<h2 id="abstract">0. Abstract</h2>
<p>Related work: GPT (Radford et al.) &amp; ELMo (Peters et al.)</p>
<blockquote>
<p>Fun fact: ELMo &amp; BERT are both character name from Sesame
Street</p>
</blockquote>
<p>Difference between these two:</p>
<ul>
<li><p>GPT - use <strong>left context</strong> for predicting future
inputs &lt;-&gt; BERT - use both context</p></li>
<li><p>ELMo - RNN architecture, need architecture modification &lt;-&gt;
BERT - Transformer Architecture, no task-specific architecture
modifications needed for downstream tasks.</p></li>
</ul>
<p>Advantages:</p>
<ul>
<li><p>Conceptually simple</p></li>
<li><p>Empirically powerful (on specific tasks)</p></li>
</ul>
<blockquote>
<p>Writing tips: State both absolute and relative performance for reader
understanding.</p>
</blockquote>
<h2 id="introduction">1. Introduction</h2>
<ul>
<li><p>Pre-training language model in NLP area (BERT reused pre-training
technique from CV area, advocating later research to follow)</p></li>
<li><p>Extension to Abstract para 1. Existing strategies for applying
pre-trained language representations</p>
<ul>
<li><p>Feature-based - ELMo</p></li>
<li><p>Fine-tuning - GPT</p></li>
</ul></li>
<li><p>Limitation on related work</p>
<ul>
<li><p>Unidirectional language models restrict pre-trained
representations.</p>
<ul>
<li>GPT’s left-to-right architecture, “every token can only at- tend to
previous tokens in the self-attention layers of the Transformer”</li>
</ul></li>
</ul></li>
<li><p>BERT’s improvement: using bi-directional context &amp; “masked
language model”</p></li>
<li><p>Contributions:</p>
<ul>
<li><p>Importance of bidirectional pre-training</p></li>
<li><p>Reducing task-specific architecture modification need</p></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/%20google-research/bert">Open-source
Repo</a></p></li>
</ul></li>
</ul>
<h2 id="related-work">2. Related Work</h2>
<h3 id="unsupervised-feature-based-approach">2.1 Unsupervised
Feature-based approach</h3>
<p>ELMo &amp; others.</p>
<h3 id="unsupervised-fine-tuning-approaches">2.2 Unsupervised
Fine-tuning Approaches</h3>
<p>GPT &amp; others.</p>
<h3 id="transfer-learning-from-supervised-data">2.3 Transfer Learning
from Supervised Data</h3>
<h2 id="bert-implementation">3. BERT (Implementation)</h2>
<ul>
<li><p>Two steps in BERT’s framework: <em>pre-training</em> and
<em>fine-tuning</em>.</p>
<blockquote>
<p>Writing tips: Include a brief introduction of supplementary
techniques used. (E.g. pre-training &amp; fine-tuning here)</p>
</blockquote></li>
</ul>
<h4 id="model-architecture">3.0.1 <strong>Model
architecture</strong></h4>
<p>Multi-layer bidirectional Transformer encoder based on Vaswani et
al.(2017) ‘s original implementation in <a
target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor">this repo</a>. Guide
could be found in <a
target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">this
article</a>.</p>
<ul>
<li><p><span class="math inline">\(BERT_{BASE} \Rightarrow
L=12,H=768,A=12,Total\ parameters=110M\)</span></p>
<p>This model is designed to have the same model size as GPT on
comparison purposes.</p></li>
<li><p><span class="math inline">\(BERT_{LARGE} \Rightarrow
L=24,H=1024,A=16,Total\ parameters=340M\)</span></p>
<p>where,</p>
<ul>
<li><span class="math inline">\(L\)</span> - number of Transformer
blocks</li>
<li><span class="math inline">\(H\)</span> - Hidden size</li>
<li><span class="math inline">\(A\)</span> - number of self-attention
heads</li>
</ul></li>
</ul>
<blockquote>
<p>In literature, bidirectional transformer is often referred to as a
“Transformer encoder”</p>
</blockquote>
<h4 id="inputoutput-representations">3.0.2 Input/Output
Representations</h4>
<ul>
<li><p>Context</p>
<p>To cope with different down-stream tasks, input representation needs
to unambiguously represent both a single <strong>sentence</strong> and a
pari of sentences in one token <strong>sequence</strong>.</p></li>
</ul>
<blockquote>
<h5 id="important-definitions">Important definitions</h5>
<ul>
<li>Sentence - arbitrary span of contiguous text, rather than an actual
linguistic sentence.</li>
<li>Sequence - input token to BERT.</li>
</ul>
</blockquote>
<ul>
<li><p>Implementation</p>
<p>WordPiece embeddings (Wu et al., 2016.) WordPiece embeddings cut word
into smaller sub-sequence for low frequency words to reduce the size of
token vocabulary.</p>
<p>Rules:</p>
<ul>
<li><p>First token of every sequence = <code>[CLS]</code> - Used for
classification tasks</p></li>
<li><p>Packing sentences</p>
<ul>
<li><p>Separate using <code>[SEP]</code> (simple mark)</p></li>
<li><p>Use a learned embedding - summing the token, segement and
position embeddings.</p>
<figure>
<img src="https://miro.medium.com/max/1400/0*m_kXt3uqZH9e7H4w.png" srcset="/img/loading.gif" lazyload
alt="BERT-paper-Figure2-BERT-input-representation" />
<figcaption
aria-hidden="true">BERT-paper-Figure2-BERT-input-representation</figcaption>
</figure></li>
</ul></li>
</ul></li>
</ul>
<h3 id="pre-training-bert">3.1 Pre-training BERT</h3>
<p>BERT’s pre-training uses two <strong>unsupervised</strong> tasks</p>
<ul>
<li>Masked Language Model (MLM)</li>
<li>Next Sentence Prediction (NSP)</li>
</ul>
<h4 id="task-i-masked-lm">3.1.1 Task I: Masked LM</h4>
<ul>
<li><p>Intuition: Bring in contextual information (as ELMo
suggests)</p></li>
<li><p>Task description</p>
<p>Mask some percentage of the input tokens at random, and then predict
those masked tokens.</p></li>
<li><p>Task details</p>
<ul>
<li>Mask 15% of all WordPiece tokens in each sequence at random.</li>
<li>To mitigate the mismatch (fine-tuning’s input has no
<code>[MASK]</code> token) between pre-training &amp; fine-tuning,
masked words are replaced in differently. If the <em>i</em>-th token is
chosen, it is replaced with
<ol type="1">
<li><code>[MASK]</code> token - 80% of the time</li>
<li>a random token - 10% of the time (to add noise into the training
data)</li>
<li>the unchanged token - 10% of the time</li>
</ol></li>
</ul></li>
</ul>
<blockquote>
<p><strong>Online demo for this task</strong></p>
<p>Check out <a
target="_blank" rel="noopener" href="https://huggingface.co/bert-base-uncased?text=The+goal+of+life+is+%5BMASK%5D.">huggingface’s
online impelmentation</a> of BERT base model.</p>
</blockquote>
<blockquote>
<p><strong>Task idea origin</strong></p>
<p>Cloze task by Taylor.</p>
</blockquote>
<h4 id="task-ii-next-sentence-prediction-nsp">3.1.2 Task II: Next
Sentence Prediction (NSP)</h4>
<ul>
<li><p>Intuition: capture sentence relationships for tasks like Question
Answering &amp; Natural Language Inference</p></li>
<li><p>Task description</p>
<p>Input two sentences A &amp; B, output a binary label indicates
whether B is the next sentence follows A.</p></li>
<li><p>Details</p>
<ul>
<li>Training data construction - 50-50 split of positive &amp; negative
samples.</li>
</ul></li>
</ul>
<h4 id="data-source">3.1.3 Data source</h4>
<blockquote>
<p>For the pre-training corpus we use the BooksCorpus (800M words) (Zhu
et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we
extract only the text passages and ignore lists, tables, and
headers.</p>
</blockquote>
<h3 id="fine-tuning-bert">3.2 Fine-tuning BERT</h3>
<p>Fine-tuning BERT is the process of reorganize the input sentence into
sequence to model different downstream tasks. For each task, plug in the
task-specific inputs and outpus is needed and BERT is finetuned
end-to-end.</p>
<blockquote>
<p>BERT’s fine-tuning is inexpensive. All of the results in the paper
can be replicated in at most 1 hour on a single Cloud TPU, or a few
hours on a GPU, starting from the exact same pre-trained model.</p>
</blockquote>
<h2 id="experiments">4. Experiments</h2>
<p>Introduces the way to cope with and the results on different
down-stream tasks. For the detail of input format modification and
experiment result, please refer to the original paper.</p>
<ul>
<li>GLUE</li>
<li>SQuAD v1.1</li>
<li>SQuAD v2.0</li>
<li>SWAG</li>
</ul>
<h2 id="ablation-studies">5. Ablation Studies</h2>
<p>In this section, ablation experiemnt over pre-training tasks, model
size and feature-based approach are performed.</p>
<p>In summary, the ablation study shows</p>
<ul>
<li>all proposed pre-training tasks are necessary for improving the
model’s performance.</li>
<li>Increasing the model size coudl lead to continual improvements on
large-scale tasks.</li>
<li>Extracting fixed features from pre-trained model experiment result
demonstrates BERT is effective for both fine-tuning &amp; feature based
approach</li>
</ul>
<blockquote>
<p>Research tips: It is good practice to perform ablation studies to add
explainability to large scale models with different components.
#Area/Research</p>
</blockquote>
<h2 id="conclusion">6. Conclusion</h2>
<ul>
<li><p>Rich, unsupervised pre-training is crucial for language
understanding.</p></li>
<li><p>Major contribution: “further generalizing these findings to deep
<em>bidirectional</em> architectures, allowing the same pre-trained
model to successfully tackle a broad set of NLP tasks.”</p></li>
<li><p>Limitation: Constrain on generation tasks as bidirectional
.</p></li>
</ul>
<h2 id="reflect">Reflect</h2>
<ol type="1">
<li>Which two former work does BERT mainly refer to? What’s the
advantage(s) of each prior work? What is BERT’s main
contribution/innovation compared to those work?</li>
<li>Which two kinds of tasks is BERT divided into?</li>
<li>Which two kinds of tasks is BERT’s pre-training divided into? What’s
the effect of each task?</li>
<li>What is the advantage and disadvantage of BERT?</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Notes/" class="category-chain-item">Notes</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Artificial-Intelligence/">#Artificial Intelligence</a>
      
        <a href="/tags/NLP/">#NLP</a>
      
        <a href="/tags/Classic-Papers/">#Classic Papers</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Literature Note - BERT</div>
      <div>https://delusion4013.github.io/2022/09/15/Literature Note - BERT/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Chenkai</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>September 15, 2022</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/11/08/A-brief-history-of-NLP/" title="A brief history of NLP">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">A brief history of NLP</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/03/13/Embrace%20the%20Information%20Era/" title="Embrace the Information Era">
                        <span class="hidden-mobile">Embrace the Information Era</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://delusion4013.github.io/2022/09/15/Literature%20Note%20-%20BERT/';
          this.page.identifier = '/2022/09/15/Literature%20Note%20-%20BERT/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'my-hexo-blog-7' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        Views: 
        <span id="busuanzi_value_site_pv"></span>
        
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        Visitors: 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
