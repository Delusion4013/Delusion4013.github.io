<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Literature Note - Transformer</title>
    <link href="/2022/11/10/Literature-Note-Transformer/"/>
    <url>/2022/11/10/Literature-Note-Transformer/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this post, I would summarize key points from classic paper <ahref="http://arxiv.org/abs/1706.03762"><Attention is all you need></a>following its initial structure.</p><span id="more"></span><blockquote><p><strong>Important Links</strong></p><ul><li><a href="http://arxiv.org/abs/1706.03762">Paper link</a></li><li><a href="https://github.com/tensorflow/tensor2tensor">Codelink</a></li></ul></blockquote><blockquote><p><strong>Acknowledgements</strong></p><p>Special thanks to Mu LI, who provides a wonderful review <ahref="https://www.bilibili.com/video/BV1pu411o7BE/">video</a> on thisarticle.</p><p>Also thanks to Lilian, for this wonderful <ahref="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">blog</a>connecting transformer's principle &amp; applications.</p></blockquote><h2 id="abstract">0. Abstract</h2><ul><li>Paper proposes a new simple network architecture, the Transformer,based solely on attention mechanisms</li><li>State-of-the-art result on <em>machine translation tasks</em></li><li>Advantages of transformer architecture<ul><li>More parallelizable &amp; require significantly less trainingtime</li><li>Generalize well to other tasks - see BERT, GPT</li></ul></li></ul><blockquote><p><strong>Writing tips</strong></p><p>When stating equal contribution using <code>*</code>, it's goodpractice to list briefly about each member's work done.</p></blockquote><h2 id="introduction">1. Introduction</h2><blockquote><p>Basically an extension to abstract</p></blockquote><ul><li><p>Recurrent neural networks /models'</p><ul><li><p><strong>Principle</strong> Generate a sequence of hidden states<span class="math inline">\(h_t\)</span>, as a function of the previoushidden state <span class="math inline">\(h_{t−1}\)</span> and the inputfor position <span class="math inline">\(t\)</span>.</p><figure><img src="https://s2.loli.net/2022/11/24/SxYCGEHjplryNwa.png"alt="RNN-calculation-example.png" /><figcaption aria-hidden="true">RNN-calculation-example.png</figcaption></figure></li><li><p><strong>Drawbacks</strong></p><ul><li>Parallelization preclusion by calculation method</li><li>High memory requirements for preserving historical information. /Possibility in forgetting early information after step by steppassing.</li></ul></li></ul></li><li><p>Attention Mechanism</p><ul><li>An integral part of sequence &amp; transduction models.</li><li>Allow the model to capture dependencies between in/out sequenceregardless of their distance.</li><li><mark>Most of them used in conjunction with a recurrentnetwork</mark></li></ul></li><li><p>Transformer's innovation</p><ul><li>“eschewing recurrence and instead relying entirely on an attentionmechanism”</li></ul></li></ul><h2 id="background">2. Background</h2><ul><li>Introduces previous attempts to reduce sequential computation -using CNN (convolutional neural networks)<ul><li>“difficult to learn dependencies between distant positions”, betweenlong sequences</li><li>Why Multi-Head Attention (instead of single head)?<ul><li>Attention mechanism eschewed convolution mechanism, losing theopportunity to model different patterns.</li><li>Using Multi-Head Attention mechanism is to <mark>simulate themulti-channel output of CNN.</mark></li></ul></li></ul></li><li>Previous success on self-attention</li><li>End-to-end memory networks’ scope and performance</li><li>Innovation point about Transformer.</li></ul><h2 id="model-architecture">3. Model Architecture</h2><p>Follows most competitive sequence transduction models, Transformeruses an <em>encoder-decoder structure</em>, where</p><ul><li>Encoder maps input sequence to a vector-like representation formodel usage.</li><li>Decoder generates output sequence one element at a time.</li></ul><p>The model is <strong>auto-regressive</strong>, using output fromprevious moment as additional input.</p><h3 id="encoder-decoder-stacks">3.1 Encoder &amp; Decoder Stacks</h3><figure><img src="https://s2.loli.net/2022/11/24/vdOCy9T3uxVQWlw.png"alt="Transformer-Architecture-from-Paper.png" /><figcaptionaria-hidden="true">Transformer-Architecture-from-Paper.png</figcaption></figure><ul><li>Encoder - Decoder, shown in the left and right halvesrespectively.</li><li>Each layer of <strong>encoder</strong> consists of two sub-layers -a multi-head attention mechanism &amp; a MLP. The residual connection(inspired by ResNet) is used within sub-layers. The<strong>LayerNorm</strong> technique is also implemented.<ul><li><strong>LayerNorm</strong> is a different normalization method fromBatch Normalization and is more suitable with models takingvariable-length inputs (temporal sequences), reducing the impact ofdifferent batch cuts (sequence length variations) on normalization.</li><li>Generally speaking, the difference lies in the data slicingmechanism. BN slices the data according to the batch and regularizes thefeature dimensions, while LN slices the data according to the inputsamples and regularizes different features of the same sample.</li></ul></li><li>In addition to encoder, decoder adds another layer, calculate themulti-head attention over the output of the encoder. A masking mechanismis used to prevent decoder uses output after position <spanclass="math inline">\(i\)</span> to predict the position <spanclass="math inline">\(i\)</span>.</li></ul><h3 id="attention">3.2 Attention</h3><p>Attention is a mechanism that model learns to <mark>make predictionsby selectively attending</mark> to a given set ofdata.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p><p>Self-attention is a type of attention mechanism where the model makesprediction for one part of a data sample using other parts of theobservation about the same sample,... it is<mark>permutation-invariant</mark>; in other words, it is an operationon sets.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p><ul><li>In transformer's Encoder, Query, Key, Value are identical vectorsgenerated by the embedding layer, therefore no trainable variablesinvolved.</li></ul><p>“An attention function can be described as mapping a query and a setof key-value pairs to an output. The output is computed as a<mark>weighted sum of the values</mark>, where the weight assigned toeach value is computed by a compatibility function of the query with thecorresponding key.”</p><p>The amount of attention is quantified by learned weights and thus theoutput is usually in the form of weighted average over input (Query,Key, Value) pair.</p><h4 id="scaled-dot-product-attention">3.2.1 Scaled Dot-ProductAttention</h4><ul><li>What's Dot-Product?<ul><li>Perform matrix multiplication on queries (Q) and keys (K)embeddings.</li></ul></li><li>Why Dot-Product?<ul><li>Faster &amp; space-efficient in practice</li></ul></li></ul><blockquote><p>Another alternative for attention functions are additiveattention.</p></blockquote><ul><li>What's Scale?<ul><li>After dot-product, divide the result by <spanclass="math inline">\(\sqrt{d_k}\)</span>.</li></ul></li><li>Why scale?<ul><li>For long input sequence, apply <code>softmax</code> on the unscaleddot-product could result in <strong>Vanishing gradient</strong>.</li></ul></li></ul><p><span class="math display">\[Attention(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</span></p><figure><img src="https://s2.loli.net/2022/11/24/4EAL3FWzb69QIpH.png"alt="Scaled Dot-Product Attention" /><figcaption aria-hidden="true">Scaled Dot-Product Attention</figcaption></figure><h4 id="multi-head-attention">3.2.2 Multi-Head Attention</h4><ul><li><p>What is Multi-head attention?</p><ul><li><img src="https://s2.loli.net/2022/11/24/aJTNOvFq7nfwRYP.png"alt="Multi-head attention illustration" /> <span class="math display">\[  \begin{aligned}  \text{MultiHead}(Q,K,V) = \text{Concat}(head_1,...,head_h)W^O \\  where\ head_i=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)  \end{aligned}  \]</span></li></ul></li><li><p>Why multi-head?</p><ul><li><blockquote><p>Multi-head attention allows the model to jointly attend toinformation from different representation subspaces at differentpositions</p></blockquote></li><li>Just like CNN's different convolution kernel, multi-head attentionenables the model to capture different input patterns, learning morerepresentations.</li></ul></li></ul><h4 id="applications-of-attention-in-our-model">3.2.3 Applications ofAttention in our Model</h4><ol type="1"><li>In "encoder-decoder attention" layers, the queries come from theprevious decoder layer, and the memory keys and values come from theoutput of the encoder.</li><li>The encoder contains self-attention layers</li><li>The decoder contains self-attention layers.</li></ol><h3 id="position-wise-feed-forward-networks">3.3 Position-wiseFeed-Forward Networks</h3><p>Put it in other way - it's a fully-connected feed-forward networkwith one hidden layer (MLP).</p><p>This layer applies a linear transformation (<spanclass="math inline">\(xW_1+b_1\)</span>) on input <code>x</code>, usinga ReLU activation function and then another linear transformation.</p><p>One thing to notice - the first transformation expand the inputdimension to <span class="math inline">\(2048\)</span>, and then projectit back in the second transformation. <span class="math display">\[    FFN(x) = \max(0, xW_1+b_1)W_2+b_2\]</span></p><ul><li>What is position-wise?<ul><li>the above linear transformation is applied to <mark>everyposition</mark> of the output separately and identically.</li><li>To put it in simple ways, it is applied to each word of the inputsequence, and is therefore called <strong>point-wise</strong></li></ul></li></ul><blockquote><p>Comparison between transformer &amp; RNN</p><p>Transformer extracts sequence information when applying attentionfunction, while the RNN directly passes the <spanclass="math inline">\(t-1\)</span> information to the next computingblock.</p></blockquote><h3 id="embeddings-and-softmax">3.4 Embeddings and Softmax</h3><ul><li>What's embedding?<ul><li>Embedding are a vector form of input token, used to representsemantic information.</li></ul></li></ul><p>In Transformer, embeddings are used for inputs of both encoder anddecoder. Same weight matrix are applied for these embeddings andpre-softmax linear transformation.</p><ul><li>Why multiply <span class="math inline">\(\sqrt{d_{model}}\)</span>in embedding layer?<ul><li>When learning embeddings, the L2 Norm value is applied. With theincrease of input dimension, the weight learnt would decrease.</li><li>However, this embedding needs to combine with the positionalencoding (with increasing integer number). To have roughly same scale ofvalue, <span class="math inline">\(\sqrt{d_{model}}\)</span> ismultiplied.</li></ul></li></ul><h3 id="positional-encoding">3.5 Positional Encoding</h3><p>As discussed in previous section, attention mechanism itself areoperations on sets. In other words, changing the order of encoder inputwould not affect its output.</p><p>To add sequence information, we applied "positional encoding". Intransformer, sine and cosine functions of different frequencies areused.</p><p><span class="math display">\[\begin{aligned}PE_{(pos,2i)}=&amp;sin(pos/10000^{2i/d_{model}})\\PE_{(pos,2i+1)}=&amp;cos(pos/10000^{2i/d_{model}})\\&amp;\text{where pos is position and i is dimension}\end{aligned}\]</span></p><h2 id="why-self-attention">4. Why self-attention?</h2><ul><li><p>Comparison between self-attention layers to the recurrent andconvolutional layers on seq2seq tasks.</p></li><li><p>Self attention is evaluated on three criteria (as followingcolumn):</p><ol type="1"><li><p>Computation speed</p></li><li><p>Amount of parallelizable computation (smaller <spanclass="math inline">\(O(x)\)</span> the better)</p></li><li><p>Path length of signals have to traverse in the network betweenlong range dependencies.</p><blockquote><p>“The shorter these paths between any combination of positions in theinput and output sequences, the easier it is to learn long-rangedependencies."</p></blockquote></li></ol></li></ul><table><thead><tr class="header"><th>Layer Type</th><th>Complexity per Layer</th><th>Sequential Operation</th><th>Maximum Path Length</th></tr></thead><tbody><tr class="odd"><td>Self-Attention</td><td><span class="math inline">\(O(n^2\cdot d)\)</span></td><td><span class="math inline">\(O(1)\)</span></td><td><span class="math inline">\(O(1)\)</span></td></tr><tr class="even"><td>Recurrent</td><td><span class="math inline">\(O(n\cdot d^2)\)</span></td><td><span class="math inline">\(O(n)\)</span></td><td><span class="math inline">\(O(n)\)</span></td></tr><tr class="odd"><td>Convolutional</td><td><span class="math inline">\(O(k\cdot n\cdot d^2)\)</span></td><td><span class="math inline">\(O(1)\)</span></td><td><span class="math inline">\(O(log_k(n))\)</span></td></tr><tr class="even"><td>Self-Attention(restricted)</td><td><span class="math inline">\(O(r\cdot n\cdot d)\)</span></td><td><span class="math inline">\(O(1)\)</span></td><td><span class="math inline">\(O(n/r)\)</span></td></tr></tbody></table><blockquote><p><span class="math inline">\(n\)</span> - input sequence length, <spanclass="math inline">\(d\)</span> - the representation dimensionality,<span class="math inline">\(k\)</span> - kernel width, <spanclass="math inline">\(r\)</span> - restricted neighborhood size</p></blockquote><ul><li><span class="math inline">\(n\)</span> is normally smaller than<span class="math inline">\(d\)</span>, giving self-attention advantagesin computing.</li><li><span class="math inline">\(k\)</span> is usually a small number,therefore CNN &amp; RNN has roughly same level of complexity.</li><li>Adding restriction to self-attention is a trade-off betweencapturing long range dependencies and computation speed.</li></ul><h2 id="training">5. Training</h2><p>In this section, training regime is described.</p><ul><li><p>Encode setting - byte-pair encoding</p></li><li><p>Dataset - larger WMT 2014 English-French dataset consisting of36M sentences</p></li><li><p>Batch - a set of sentence pairs with approximately 25000 sourcetokens and 25000 target tokens</p></li><li><p>Hardware - 8 NVIDIA P100 GPUs (still affordable, compared to GPT:P)</p><ul><li>TPU is implemented afterwards to facilitate transformer likecomputation (with large matrix multiplication)</li></ul></li><li><p>Time</p><ul><li>Base model - 0.4s step time,100,000 steps / 12 hours</li><li>big model - 1.0s step time, 300,000 steps / 3.5 days</li></ul></li><li><p>Optimizer</p><ul><li>Adam, <spanclass="math inline">\(\beta_1=0.9,\beta_2=0.98,\epsilon=10^{-9}\)</span></li></ul></li><li><p>Scheduling</p><ul><li><span class="math inline">\(lrate = d^{-0.5}_{model} \cdotmin(step\_num^{-0.5}, step\_ num \cdotwarmup\_steps^{-1.5})\)</span></li><li>"This corresponds to increasing the learning rate linearly for thefirst <code>warmup_steps</code> training steps, and decreasing itthereafter proportionally to the inverse square root of the step number.We used warmup_steps = 4000."</li></ul></li><li><p>Regularization</p><ul><li>Residual dropout<ul><li>output of each sub-layer, sums of the embeddings and the positionalencodings in both the encoder and decoder stacks.</li><li><span class="math inline">\(P_{drop}=0.1\)</span></li></ul></li><li>Label smoothing<ul><li>value <span class="math inline">\(\epsilon_{ls}=0.1\)</span></li><li>hurts perplexity, but improves accuracy &amp; BLUE score.</li></ul></li></ul></li></ul><h2 id="results">6. Results</h2><p>In this section, results on machine translation (English-to-German),different model architecture, English Constituency Parsing tasks aresummarized.</p><h2 id="conclusion">7. Conclusion</h2><ul><li>Transformer - "The first sequence transduction model based entirelyon attention"</li><li>Achieved a new state of the art on machine translation task</li><li>Proposing a new model architecture for handling multi modalities&amp; adapting to different downstream tasks (see more in BERT)</li></ul><div id="footnotes"><hr><div id="footnotelist"><ol><li id="fn:1"><a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">TheTransformer Family | Lil'Log(lilianweng.github.io)</a><a href="#fnref:1" rev="footnote"> ↩︎</a></li></ol></div></div>]]></content>
    
    
    <categories>
      
      <category>Computer Science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Artificial Intelligence</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A brief history of NLP</title>
    <link href="/2022/11/08/A-brief-history-of-NLP/"/>
    <url>/2022/11/08/A-brief-history-of-NLP/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this piece of note, I will give an overview of NLP's developmenthistory, focusing on how recent neural approaches revolutionise the NLPfield.</p><span id="more"></span><p>Natural language processing could be roughly devided into 3 stagesbased on the domainance methods used.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p><ul><li>Symbolic NLP</li><li>Statitical NLP</li><li>Neural NLP</li></ul><h3 id="symbolic-nlp-1950s---early-1990s">Symbolic NLP (1950s - early1990s)</h3><p>Research related to natural language processing originated roughly inthe 1950s. In the following 40 years, limited by the size of the<strong>corpus</strong> and the <strong>computing power</strong>, earlynatural language processing mainly used <strong>rule-basedmethods</strong> to deal with generic natural language phenomena throughsymbolic logic knowledge summarized by experts. Such rule systems aredifficult to be applied to solve real-world problems due to thecomplexity of natural language.</p><h3 id="statistical-nlp-1990s---2010s">Statistical NLP (1990s -2010s)</h3><p>The rapid <strong>advances in computational power and storagecapacity</strong>, as well as the increasing maturity of statisticallearning approaches, have led to the large-scale application ofcorpus-based statistical learning methods in the field of naturallanguage processing.</p><p>The advantages of this method includes fast training speed, littlerequirement for labelled data and has good performance over simpleproblems.</p><p>Meanwhile, there are obvious limitations to this method. Statisticalapproach requires transformation of the raw natural language input intoa vector form that can be processed by the machine based on empiricalrules. This expertise-dependent, manual process of transformation isknown as feature engineering (feature extraction), which istime-consuming and not compatible for different tasks.</p><h3 id="neural-nlp-present">Neural NLP (present)</h3><p>To cope with the disadvantages of feature engineering, representationlearning and deep neural network-style machine leanring methods becamewidely applied in NLP field, proposing an end-to-end solution.Representation learning allows the machine to automatiacally recognisepatterns from input which can be used for tasks like classifcation.</p><p>In 2013, Tomas Mikolov proposed <strong>word2vec</strong>method<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>,using a shallow neural network with large scale corpus, which usescontextual connection of each word to embed the semantics of the tokensinto a dense vector. This output of such method is called <strong>WordEmbedding</strong>.</p><p>This kinds of encoding methods avoid the usage of elaborate featureengineering, and it also breaks down the barriers between differenttasks, as representation learning transformes input into a similar andeasily tranferrable vector space.</p><blockquote><p>This trend of Representation Learning has spread to knowledge graphs(using Graph Embedding techniques) and recommender systems (usinguser/item Embedding techniques).</p></blockquote><p>Shortly after, the drawback of word2vec was discovered - the sameword has different meanings in different contexts, but the word vectorgiven by this encoding is unique and static. Accordingly, the model<strong>ELMo</strong> that introduces contextual Word Embedding wasborn.</p><p>In 2017, the Transformer model wasreleased<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>.Compared to ELMo-like models, the biggest breakthrough of Transformer isthat it does not use LSTM, but instead uses an <strong>attentionmechanism</strong>. This mechanism is a function that maps a query and aset of key-value pairs to an output. The values output by the attentionmechanism are weighted sums, where the weight of each value iscalculated by the function of the query and the corresponding key of thevalue.</p><blockquote><p>Some NLP researchers believe that the attention mechanism used bytransformer is a better alternative to LSTM. They believe that theattention mechanism handles long-range dependencies better than LSTM andhas a very promising application. transformer uses an encoder-decoderstructure in its architecture. The encoder and decoder are highlysimilar in structure, but not in function. The encoder consists of Nidentical encoder layers. A decoder consists of N identical decoderlayers. Both encoder and decoder layers use the attention mechanism as acore component.</p></blockquote><p>The great success of Transformer in the field of machine translationhas attracted the interest of many NLP scientists. As deep learningalgorithms evolves, their disadvantages begin to emerge - algorithmrequires massive labelled data. As the subjective natrue of congnitivetask of NLP, and the large number of tasks and domains it deals with, itis time-consuming and labor-intensive for acquiring high qualityannotated corpora. The large scale pre-trained langauge model preciselycompensate for this shortage, helping NLP to achieve a series ofbreakthroughs. On this regard, two famous models were born:Bidirectional Encoder Representations from Transformers(BERT)<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> andGenerative Pre-Traing of language model(GPT)<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>.</p><p>GPT consists entirely of the decoder layer of the transformer, whileBERT consists entirely of the encoder layer of the transformer. The goalof GPT is to generate human-like text, the goal of BERT, on the otherhand, is to provide better language representations that help achievebetter results for a wide range of downstream tasks. The BERT modelreaches an advanced level on a variety of NLP tasks and have greatlyimproved STOA on many tasks. Now BERT has derived a large family ofmodels, among which the famous ones are XL-Net, RoBERTa,ALBERT<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>,ELECTRA, ERNIE, BERT-wwm, DistillBERT, etc.</p><div id="footnotes"><hr><div id="footnotelist"><ol><li id="fn:1"><a href="https://en.wikipedia.org/wiki/Natural_language_processing#History">Naturallanguage processing#History -Wikipedia</a><a href="#fnref:1" rev="footnote"> ↩︎</a></li><li id="fn:2"><a href="https://en.wikipedia.org/wiki/Word2vec">Word2vec -Wikipedia</a><a href="#fnref:2" rev="footnote"> ↩︎</a></li><li id="fn:3"><a href="https://arxiv.org/abs/1706.03762">「1706.03762」 Attention IsAll You Need (arxiv.org)</a><a href="#fnref:3" rev="footnote"> ↩︎</a></li><li id="fn:4"><a href="https://arxiv.org/abs/1810.04805">「1810.04805」 BERT:Pre-training of Deep Bidirectional Transformers for LanguageUnderstanding (arxiv.org)</a><a href="#fnref:4" rev="footnote"> ↩︎</a></li><li id="fn:5"><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">ImprovingLanguage Understanding by GenerativePre-Training</a><a href="#fnref:5" rev="footnote"> ↩︎</a></li><li id="fn:6"><a href="https://arxiv.org/abs/1909.11942">「1909.11942」 ALBERT: A LiteBERT for Self-supervised Learning of Language Representations(arxiv.org)</a><a href="#fnref:6" rev="footnote"> ↩︎</a></li></ol></div></div>]]></content>
    
    
    <categories>
      
      <category>Computer Science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Artificial Inteliigence</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Literature Note - BERT</title>
    <link href="/2022/09/15/Literature%20Note%20-%20BERT/"/>
    <url>/2022/09/15/Literature%20Note%20-%20BERT/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this post, I would summarize key points from classic paper <ahref="http://arxiv.org/abs/1810.04805">&lt;BERT: Pre-training of DeepBidirectional Transformers for Language Understanding&gt;</a> followingits initial structure.</p><span id="more"></span><blockquote><p>Important Links</p><ul><li><a href="http://arxiv.org/abs/1810.04805">Paper</a></li><li><a href="https://github.com/google-research/bert">OfficialCode</a></li></ul></blockquote><h2 id="abstract">0. Abstract</h2><p>Related work: GPT (Radford et al.) &amp; ELMo (Peters et al.)</p><blockquote><p>Fun fact: ELMo &amp; BERT are both character name from SesameStreet</p></blockquote><p>Difference between these two:</p><ul><li><p>GPT - use <strong>left context</strong> for predicting futureinputs &lt;-&gt; BERT - use both context</p></li><li><p>ELMo - RNN architecture, need architecture modification &lt;-&gt;BERT - Transformer Architecture, no task-specific architecturemodifications needed for downstream tasks.</p></li></ul><p>Advantages:</p><ul><li><p>Conceptually simple</p></li><li><p>Empirically powerful (on specific tasks)</p></li></ul><blockquote><p>Writing tips: State both absolute and relative performance for readerunderstanding.</p></blockquote><h2 id="introduction">1. Introduction</h2><ul><li><p>Pre-training language model in NLP area (BERT reused pre-trainingtechnique from CV area, advocating later research to follow)</p></li><li><p>Extension to Abstract para 1. Existing strategies for applyingpre-trained language representations</p><ul><li><p>Feature-based - ELMo</p></li><li><p>Fine-tuning - GPT</p></li></ul></li><li><p>Limitation on related work</p><ul><li><p>Unidirectional language models restrict pre-trainedrepresentations.</p><ul><li>GPT’s left-to-right architecture, “every token can only at- tend toprevious tokens in the self-attention layers of the Transformer”</li></ul></li></ul></li><li><p>BERT’s improvement: using bi-directional context &amp; “maskedlanguage model”</p></li><li><p>Contributions:</p><ul><li><p>Importance of bidirectional pre-training</p></li><li><p>Reducing task-specific architecture modification need</p></li><li><p><a href="https://github.com/%20google-research/bert">Open-sourceRepo</a></p></li></ul></li></ul><h2 id="related-work">2. Related Work</h2><h3 id="unsupervised-feature-based-approach">2.1 UnsupervisedFeature-based approach</h3><p>ELMo &amp; others.</p><h3 id="unsupervised-fine-tuning-approaches">2.2 UnsupervisedFine-tuning Approaches</h3><p>GPT &amp; others.</p><h3 id="transfer-learning-from-supervised-data">2.3 Transfer Learningfrom Supervised Data</h3><h2 id="bert-implementation">3. BERT (Implementation)</h2><ul><li><p>Two steps in BERT’s framework: <em>pre-training</em> and<em>fine-tuning</em>.</p><blockquote><p>Writing tips: Include a brief introduction of supplementarytechniques used. (E.g. pre-training &amp; fine-tuning here)</p></blockquote></li></ul><h4 id="model-architecture">3.0.1 <strong>Modelarchitecture</strong></h4><p>Multi-layer bidirectional Transformer encoder based on Vaswani etal.(2017) ‘s original implementation in <ahref="https://github.com/tensorflow/tensor2tensor">this repo</a>. Guidecould be found in <ahref="http://nlp.seas.harvard.edu/2018/04/03/attention.html">thisarticle</a>.</p><ul><li><p><span class="math inline">\(BERT_{BASE} \RightarrowL=12,H=768,A=12,Total\ parameters=110M\)</span></p><p>This model is designed to have the same model size as GPT oncomparison purposes.</p></li><li><p><span class="math inline">\(BERT_{LARGE} \RightarrowL=24,H=1024,A=16,Total\ parameters=340M\)</span></p><p>where,</p><ul><li><span class="math inline">\(L\)</span> - number of Transformerblocks</li><li><span class="math inline">\(H\)</span> - Hidden size</li><li><span class="math inline">\(A\)</span> - number of self-attentionheads</li></ul></li></ul><blockquote><p>In literature, bidirectional transformer is often referred to as a“Transformer encoder”</p></blockquote><h4 id="inputoutput-representations">3.0.2 Input/OutputRepresentations</h4><ul><li><p>Context</p><p>To cope with different down-stream tasks, input representation needsto unambiguously represent both a single <strong>sentence</strong> and apari of sentences in one token <strong>sequence</strong>.</p></li></ul><blockquote><h5 id="important-definitions">Important definitions</h5><ul><li>Sentence - arbitrary span of contiguous text, rather than an actuallinguistic sentence.</li><li>Sequence - input token to BERT.</li></ul></blockquote><ul><li><p>Implementation</p><p>WordPiece embeddings (Wu et al., 2016.) WordPiece embeddings cut wordinto smaller sub-sequence for low frequency words to reduce the size oftoken vocabulary.</p><p>Rules:</p><ul><li><p>First token of every sequence = <code>[CLS]</code> - Used forclassification tasks</p></li><li><p>Packing sentences</p><ul><li><p>Separate using <code>[SEP]</code> (simple mark)</p></li><li><p>Use a learned embedding - summing the token, segement andposition embeddings.</p><figure><img src="https://miro.medium.com/max/1400/0*m_kXt3uqZH9e7H4w.png"alt="BERT-paper-Figure2-BERT-input-representation" /><figcaptionaria-hidden="true">BERT-paper-Figure2-BERT-input-representation</figcaption></figure></li></ul></li></ul></li></ul><h3 id="pre-training-bert">3.1 Pre-training BERT</h3><p>BERT’s pre-training uses two <strong>unsupervised</strong> tasks</p><ul><li>Masked Language Model (MLM)</li><li>Next Sentence Prediction (NSP)</li></ul><h4 id="task-i-masked-lm">3.1.1 Task I: Masked LM</h4><ul><li><p>Intuition: Bring in contextual information (as ELMosuggests)</p></li><li><p>Task description</p><p>Mask some percentage of the input tokens at random, and then predictthose masked tokens.</p></li><li><p>Task details</p><ul><li>Mask 15% of all WordPiece tokens in each sequence at random.</li><li>To mitigate the mismatch (fine-tuning’s input has no<code>[MASK]</code> token) between pre-training &amp; fine-tuning,masked words are replaced in differently. If the <em>i</em>-th token ischosen, it is replaced with<ol type="1"><li><code>[MASK]</code> token - 80% of the time</li><li>a random token - 10% of the time (to add noise into the trainingdata)</li><li>the unchanged token - 10% of the time</li></ol></li></ul></li></ul><blockquote><p><strong>Online demo for this task</strong></p><p>Check out <ahref="https://huggingface.co/bert-base-uncased?text=The+goal+of+life+is+%5BMASK%5D.">huggingface’sonline impelmentation</a> of BERT base model.</p></blockquote><blockquote><p><strong>Task idea origin</strong></p><p>Cloze task by Taylor.</p></blockquote><h4 id="task-ii-next-sentence-prediction-nsp">3.1.2 Task II: NextSentence Prediction (NSP)</h4><ul><li><p>Intuition: capture sentence relationships for tasks like QuestionAnswering &amp; Natural Language Inference</p></li><li><p>Task description</p><p>Input two sentences A &amp; B, output a binary label indicateswhether B is the next sentence follows A.</p></li><li><p>Details</p><ul><li>Training data construction - 50-50 split of positive &amp; negativesamples.</li></ul></li></ul><h4 id="data-source">3.1.3 Data source</h4><blockquote><p>For the pre-training corpus we use the BooksCorpus (800M words) (Zhuet al., 2015) and English Wikipedia (2,500M words). For Wikipedia weextract only the text passages and ignore lists, tables, andheaders.</p></blockquote><h3 id="fine-tuning-bert">3.2 Fine-tuning BERT</h3><p>Fine-tuning BERT is the process of reorganize the input sentence intosequence to model different downstream tasks. For each task, plug in thetask-specific inputs and outpus is needed and BERT is finetunedend-to-end.</p><blockquote><p>BERT’s fine-tuning is inexpensive. All of the results in the papercan be replicated in at most 1 hour on a single Cloud TPU, or a fewhours on a GPU, starting from the exact same pre-trained model.</p></blockquote><h2 id="experiments">4. Experiments</h2><p>Introduces the way to cope with and the results on differentdown-stream tasks. For the detail of input format modification andexperiment result, please refer to the original paper.</p><ul><li>GLUE</li><li>SQuAD v1.1</li><li>SQuAD v2.0</li><li>SWAG</li></ul><h2 id="ablation-studies">5. Ablation Studies</h2><p>In this section, ablation experiemnt over pre-training tasks, modelsize and feature-based approach are performed.</p><p>In summary, the ablation study shows</p><ul><li>all proposed pre-training tasks are necessary for improving themodel’s performance.</li><li>Increasing the model size coudl lead to continual improvements onlarge-scale tasks.</li><li>Extracting fixed features from pre-trained model experiment resultdemonstrates BERT is effective for both fine-tuning &amp; feature basedapproach</li></ul><blockquote><p>Research tips: It is good practice to perform ablation studies to addexplainability to large scale models with different components.#Area/Research</p></blockquote><h2 id="conclusion">6. Conclusion</h2><ul><li><p>Rich, unsupervised pre-training is crucial for languageunderstanding.</p></li><li><p>Major contribution: “further generalizing these findings to deep<em>bidirectional</em> architectures, allowing the same pre-trainedmodel to successfully tackle a broad set of NLP tasks.”</p></li><li><p>Limitation: Constrain on generation tasks as bidirectional.</p></li></ul><h2 id="reflect">Reflect</h2><ol type="1"><li>Which two former work does BERT mainly refer to? What’s theadvantage(s) of each prior work? What is BERT’s maincontribution/innovation compared to those work?</li><li>Which two kinds of tasks is BERT divided into?</li><li>Which two kinds of tasks is BERT’s pre-training divided into? What’sthe effect of each task?</li><li>What is the advantage and disadvantage of BERT?</li></ol>]]></content>
    
    
    <categories>
      
      <category>Computer Science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Artificial Inteliigence</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Embrace the Information Era</title>
    <link href="/2022/03/13/Embrace%20the%20Information%20Era/"/>
    <url>/2022/03/13/Embrace%20the%20Information%20Era/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Though it may sound strange to start embrace the Information Era tillnow, I wanted to record some of my realizations regarding how to dealwith the massive incoming information and how to gain &amp; useinformation effectively. <span id="more"></span></p><p>When I talk about Information Era, I am not referring to the timewhen information technology explodes, but after that, when we areoverwhelmed by massive information. According to relevant study, moderncitizens percieve one million times more information than citizens backin 1000 A.D. The development of all medium and the widespread ofpersonal computer contributes to the astonishing increase of informationavailable. Therefore, I would like to focus on this aspect anddemonstrate my understanding and methodology.</p><h2 id="understanding-of-information-era">Understanding of Informationera</h2><h3 id="what-is-information">What is information?</h3><blockquote><p>Information is from which <strong>data</strong> and<strong>knowledge</strong> can be derived, as data represents valuesattributed to parameters, and knowledge signifies understanding of realthings or abstract concepts.</p></blockquote><h3 id="characteristic-of-information">Characteristic ofinformation</h3><ol type="1"><li><p>Information is <strong>infinite</strong>,<strong>quickly-updated</strong></p><p>Compare to the limited time and effort each individual possess, themassive information generated each day and accumulated in the past seemsinfinite.</p></li><li><p>Different information may be <strong>contradicting</strong></p><p>Due to different perspective and former knowledge, ones could havedifferent views on the same thing. For example, in stock market, it iscommon that a lot of reviewers have contradicting views on whether themarket is bearish or bullish.</p></li><li><p>The capture &amp; storage of information <strong>takes time andeffort</strong></p><p>According to its definition, understanding and extracting the usefulpiece of information need human’s time and effort. Besides, the form ofinformation varies, such as videos, articles, audios, etc. Handlingdifferent forms of information also requires individual’s time andeffort.</p></li></ol><h2 id="methodology-in-this-era">Methodology in this era</h2><p>Regarding to the <ahref="###%20Characteristic%20of%20information">characteristic ofinformation</a>, I have formed the below methodology.</p><ol type="1"><li><p>Be <strong>selective</strong> and make decision with sufficientinformation</p><p>As the information is infinite, we need to be astute when capturinginformation. This involves finding relevant information quickly (usingsearch engine) and selecting those important ones that is most suitablefor current situation.</p><p>The thought of coming with a ‘perfect’ plan should be abandoned.Since the available information is infinite, you could always use othersource to perfect your plan further. The best way to do this would bequickly draft a plan, starting implement it and continue with iterativeperfection based on the feedback from your practice.</p></li><li><p>Develop critical thinking and make independent judgments</p><p>Due to the possible conflicting views from different informationsource, you need to develop the ability of critical thinking. You shouldevaluate views from different sides, find their reasoning and process,and then based on your experience and knowledge to make a informeddecision.</p></li><li><p>Form your own workflow for the capture, organize and induction ofinformation</p></li></ol><h3 id="capture-info-search-engine-techniques">Capture info: SearchEngine Techniques</h3><p>According to research, 90% of the people didn’t realize the belowusage of search engine. You could customize your search via simpleformatting of your query, which would largely reduce the time you findrelevant information.</p><ol type="1"><li><p>Search for key phrases <code>“”</code></p><p>Wrap up your phrase with <code>“”</code> to search the entire phraserather than search word by word.</p><figure><img src="https://s2.loli.net/2022/03/16/SxZatqpoQGDlzmE.png"alt="search-engine-keyprases-eg1.png" /><figcaptionaria-hidden="true">search-engine-keyprases-eg1.png</figcaption></figure><figure><img src="https://s2.loli.net/2022/03/16/wG9clIuJDi1SdLj.png"alt="search-engine-keypharse-eg2" /><figcaption aria-hidden="true">search-engine-keypharse-eg2</figcaption></figure></li><li><p>Exclude key phrases <code>-</code></p><p>Use a <code>-</code> before the keyword you would like toexclude.</p><figure><img src="https://s2.loli.net/2022/03/16/dkmxwOJf98LBrA6.png"alt="search-engine-exclude-eg1" /><figcaption aria-hidden="true">search-engine-exclude-eg1</figcaption></figure><figure><img src="https://s2.loli.net/2022/03/16/4zAqsKpiBSGJEme.png"alt="image-20220315224450069" /><figcaption aria-hidden="true">image-20220315224450069</figcaption></figure></li><li><p>Searching for synonyms <code>~</code></p><p>Place the <code>~</code> in front of the search term.</p></li><li><p>Using <strong>AND</strong> <strong>OR</strong> logic</p><p>Place keyword <code>AND</code> <code>OR</code> between search termsto indicate whether you want all of them or part of them to appear inthe search result.</p></li><li><p>Using <strong>grammar</strong> to filter result</p></li></ol><ul><li><p>Suppose you want to find something in <strong>a particularwebsite</strong>, use the keyword <code>site:</code>. E.g.<code>Awesome Computer Science Courses site:github.com</code></p></li><li><p>Suppose you want to find something in <strong>specific filetype</strong>, use the keyword <code>filetype:</code>. E.g.<code>How to take smart notes filetype:pdf</code></p></li><li><p>Suppose you want to find <strong>related websites</strong> (havesimilar content), use the keyword <code>related:</code>. E.g.</p><figure><img src="https://s2.loli.net/2022/03/16/YeXLzC4xRskhfPD.png"alt="image-20220315220125781" /><figcaption aria-hidden="true">image-20220315220125781</figcaption></figure></li><li><p>Suppose you want your search term to appear in the title of thesearch result, use the keyword <code>intitle:</code>.</p></li></ul><h3 id="organizing-info-categorization-and-using-tools">Organizing info:Categorization and Using tools</h3><h4 id="categorization">Categorization</h4><p>Inspiration of this idea comes from David Allen’s book introducingGTD method. One of the principle in GTD is to collect all the things(to-dos, reference materials, action reminder, etc.) in one place andcategorize them later to maintain organized. It is the same fororganizing the information you captured.</p><p>To make use of them later, you should categorize them into differentcategories based on its type (video or article) or its topic. Thecategorization process would help you find relevant informationlater.</p><h4 id="literature-management-tools">Literature management tools</h4><p>As for me, I like to keep a record of the original copy of theinformation source, such as a journal article or a video, for laterreview. Therefore the literature management tools (in my case, Zotero)help me a lot.</p><p>With Zotero (or other similar tools like Citavi, endnote), youcould</p><ul><li>Capture journal articles using DOIs or ISBN, and websites or blogthrough browser extension.</li><li>Cite your reference in Microsoft Word, OpenOffice, LibreOffice,etc.</li><li>Sync your references cross-platform</li><li>Manage your notes for different entries</li></ul><h3 id="extracting-info-knowledge-management">Extracting info: Knowledgemanagement</h3><p>Only digested information is valuable and reusable. Another name forthis would be knowledge. Sometimes you need to express the informationin your own language to see if you understand them fully (Feynman methodof learning).</p><p>In AI field, knowledge management is implemented using Triples(<code>(Subject,predicate,object)</code> such as<code>(wheel, part of, cars)</code>) and reasoning rules, and thesebasic units together forms a larger semantic web or knowledge graph.This is some how instructive on how you manage your personal knowledge-&gt; make each note atomic and then forms a larger repository.</p><p>As for individuals, managing reusable knowledge requires note takingstrategies. A field called PKM (Personal Knowledge management) focuseson solve this issue. I would discuss my understanding of PKM and mymethodology in a later article.</p>]]></content>
    
    
    <categories>
      
      <category>Informal Essays</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Methodology</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Introduction to Cryptography</title>
    <link href="/2022/02/17/Introduction-to-Crypotography/"/>
    <url>/2022/02/17/Introduction-to-Crypotography/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this piece of note, I will give an overview of Crypotography,introduce basic principles and algorithms for symmetric cryptography,assymetric cryptography and Protocols.</p><span id="more"></span><h1 id="structure-of-cryptography">Structure of Cryptography</h1><figure><img src="https://s2.loli.net/2022/02/18/SMT17RiAEoOHYmv.png"alt="Structure of Cryptography" /><figcaption aria-hidden="true">Structure of Cryptography</figcaption></figure><h2 id="symmetric-cryptography">Symmetric Cryptography</h2><blockquote><p><strong>Same key</strong> for encryption and decryption</p></blockquote><ul><li>Ensures <strong>confidentiality</strong></li><li>Implemented via <strong>block ciphers</strong> or <strong>streamciphers</strong><ul><li>Lightweight and fast</li><li>Used for general communication</li></ul></li></ul><h3 id="stream-ciphers">Stream Ciphers</h3><h4 id="implementation">Implementation</h4><ul><li><p>Initial <strong>seed</strong> key to generate an <strong>infinitekeystream</strong> of random bits</p></li><li><p>Using same keystream to encrypt two messages -&gt; easy tobreak</p><ul><li>A random <em>“number used once”</em> ( <strong>nonce</strong> )added as additional seed -&gt; ensure keystream is new</li></ul></li><li><p>Message &amp; keystream combined using <strong>XOR</strong> toget the cipher text</p><ul><li><mark>XOR is <strong>reversible</strong> is applied twice</mark>,which brings much convenience when decrypt cipher text.</li></ul></li></ul><h4 id="advantages">Advantages</h4><ul><li>Encrypting <strong>long continuous streams</strong>, possibly ofunknown length</li><li>Extremely <strong>fast</strong> with a <strong>low memory</strong>footprint, ideal for low-power devices</li><li>If designed well, can <strong>seek</strong> to any location in thestream</li></ul><h4 id="disadvantages">Disadvantages</h4><ul><li>The keystream must <strong>appear statistically random</strong></li><li>You must *<strong>never* reuse a key + nonce</strong></li><li>Stream ciphers <strong>do not protect the ciphertext</strong><ul><li>Therefore, message could be manipulated during transition withoutbreaking confidentiality</li><li>E.g., suppose you are transmitting a message to bank saying A owesyou $50. Attacker could either manipulate the amount or the creditorusing the same stream ciper or resent the same message to server.</li></ul></li></ul><h3 id="block-cipers">Block Cipers</h3><h4 id="implementation-1">Implementation</h4><ul><li>Use a key to encrypt a <strong>fixed-size block</strong> ofplaintext into a <strong>fixed-size block</strong> of ciphertext<ul><li>Changing and permuting the bits of the block depending on thekey</li></ul></li><li>Different lengths of messages can be handled by splitting themessage up and padding</li></ul><h4 id="example---sp-networks">Example - SP-Networks</h4><p><ahref="https://en.wikipedia.org/wiki/Substitution–permutation_network">Wiki</a></p><ul><li><p>Repeated substitution and permutation</p><figure><img src="https://s2.loli.net/2022/02/18/DdKNjiPcCJvgkhW.png"alt="SP-Network Example" /><figcaption aria-hidden="true">SP-Network Example</figcaption></figure></li><li><p>Key mixing for enhancing security -&gt; Different key fordifferent round</p></li><li><p>Decipher = Reverse operation</p></li></ul><h3 id="symmetric-algorithms">Symmetric Algorithms</h3><table><thead><tr class="header"><th>Algorithm</th><th>Cipher Type</th><th>Design</th><th>Block Size (bits)</th><th>Speed</th><th>Memory Footprint</th><th>Safe Implementation Difficulty</th><th>Key Sizes (bits)</th></tr></thead><tbody><tr class="odd"><td>DES</td><td>Block</td><td>Feistel</td><td>64</td><td>Fast</td><td>Low</td><td>Easy</td><td>56</td></tr><tr class="even"><td>3DES</td><td>Block</td><td>Feistel</td><td>64</td><td>Slow</td><td>Low</td><td>Easy</td><td>112</td></tr><tr class="odd"><td>AES</td><td>Block</td><td>SP-Network</td><td>128</td><td>Very fast</td><td>Low-Medium</td><td>Hard</td><td>128/192/256</td></tr><tr class="even"><td>ChaCha20</td><td>Stream</td><td>add-xor-rot</td><td>N/A</td><td>Very fast</td><td>Very low</td><td>Easy</td><td>256</td></tr></tbody></table><h2 id="asymmetric-cryptography">Asymmetric Cryptography</h2><blockquote><p>Use a <strong>pair</strong> of keys, one public and one private</p></blockquote><h3 id="public-key-cryptography">Public-key cryptography</h3><h4 id="general-idea">General Idea</h4><ul><li>Hinges upon the premise that: <em>It is computationally infeasibleto calculate a private from a public key</em></li><li>In practice, it is achieved through <strong>intractable mathematicalproblem</strong></li></ul><h4 id="key-exchange">Key exchange</h4><ul><li><p>Diffie-Hellman Key exchange allows two parties to mathematicallyagree a shared secret over an insecure channel</p></li><li><p>Alice and Bob each uses a public non-reversible Generator withtheir private keys to generate public key and send it to each other.Using other’s public key and self’s private key, shared secret could beestablished.</p><figure><img src="https://s2.loli.net/2022/02/18/S5Ud1z7g3yN2aPl.jpg"alt="Asymmetric-Cryptography-key-exchange" /><figcaptionaria-hidden="true">Asymmetric-Cryptography-key-exchange</figcaption></figure></li></ul><h4 id="public-key-encryption">Public Key Encryption</h4><ul><li>Encryption performed by the <strong>public key</strong> can only bereversed using the <strong>private key</strong></li></ul><h4 id="digital-signatures">Digital Signatures</h4><ul><li>The authenticity of signatures generated by the private key can byverified by the public key</li><li>Steps<ol type="1"><li>Server send the original message</li><li>Server use private key to encrypt</li><li>Server send the encrypted message</li><li>User verify using public key</li></ol></li></ul><h3 id="public-key-algorithms">Public Key Algorithms</h3><table><thead><tr class="header"><th>Algorithm</th><th>Key Exchange</th><th>Encryption</th><th>Digital Signatures</th><th>Mathematical Problem</th><th>Elliptic Curves?</th><th>Typical key Size (bits)</th><th></th></tr></thead><tbody><tr class="odd"><td>Diffie-Hellman</td><td>✓</td><td></td><td></td><td>Discrete Logs</td><td>✓</td><td>256</td><td></td></tr><tr class="even"><td>RSA</td><td></td><td>✓</td><td>✓</td><td>Integer Factorisation</td><td></td><td>2048/4096</td><td></td></tr><tr class="odd"><td>Elgamal</td><td></td><td>✓</td><td>✓</td><td>Discrete Logs</td><td>✓</td><td>2048</td><td></td></tr><tr class="even"><td>DSA</td><td></td><td></td><td>✓</td><td>Discrete Logs</td><td>✓</td><td>256</td><td></td></tr></tbody></table><h2 id="protocols">Protocols</h2><blockquote><p>Application of cryptographic algorithms in secure systems</p></blockquote><h3 id="hash-functions">Hash Functions</h3><ul><li><p>Cryptographic primitive</p></li><li><p>Takes a message of any length, and returns a pseudorandom hash offixed length</p></li><li><p>Strong hash functions</p><ul><li><p>must appear <strong>random</strong></p></li><li><p>be hard to find collisions – two messages that hash to the samething</p><figure><img src="https://s2.loli.net/2022/02/18/tmOYN6ISBkEfVL5.jpg"alt="strong hash funciton example" /><figcaption aria-hidden="true">strong hash funciton example</figcaption></figure></li></ul><p>### Hash Function usage</p></li></ul><h4 id="message-authentication-codes">Message Authentication Codes</h4><ul><li><p>Provide <strong>integrity</strong> and<strong>authenticity</strong>, not confidentiality</p><ul><li>Protecting system files</li><li>Ensuring messages haven’t been altered</li></ul></li><li><p>Calculate a keyed hash of the message, then append to the end ofthe message</p><figure><img src="https://s2.loli.net/2022/02/18/osuM17TI5D4x2pX.png"alt="hash funciton in message authentication" /><figcaption aria-hidden="true">hash funciton in messageauthentication</figcaption></figure></li></ul><h4 id="digital-signatures-1">Digital Signatures</h4><ul><li>The use of a hash during the signing process shortens thesignature</li><li>More efficient for long messages</li></ul><h4 id="password-storage">Password storage</h4><ul><li>Passwords stored hashed to prevent disclosure</li></ul>]]></content>
    
    
    <categories>
      
      <category>Computer Science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Cryptography</tag>
      
      <tag>Note</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thinking about Time management</title>
    <link href="/2022/02/16/Thinking-about-time-management/"/>
    <url>/2022/02/16/Thinking-about-time-management/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this article, I will talk about my understanding andimplementation of time management measures. Moreover, I would introducesome popular concepts of this topic, like GTD, Pomodoros,The EisenhowerMethod. <span id="more"></span></p><h2 id="understanding-of-time-management">Understanding of Timemanagement</h2><blockquote><p>What it is? What’s its importance?</p></blockquote><p>Time management is the process of carefully planning time individualsspent on various activities. The aim of the time management is toincrease efficiency and productivity. For a simple example, suppose youneed to prepare dinner for your family, you need to complete a set oftasks like shopping for groceries, preparing ingridients, cooking, etc.Time management skills tend to help you reduce the overall time spent onall this tasks by arranging them consciously.</p><p>In computer science, the design of operating system provides anabstraction on this topic via CPU scheduling.</p><ul><li>The priorities of tasks resembles the real life situation where sometasks is more urgent than others.</li><li>The overhead of context switch (switching between tasks) is a vividabstraction of human changing focus.</li><li>For a difficult task (takes more time), it was split into more timeslices to complete.</li></ul><h2 id="popular-concepts">Popular Concepts</h2><h3 id="the-eisenhower-method">The Eisenhower Method</h3><p>It is a method that utilizes the criteria of importance and urgencyto organize priorities and workload. Based on the two criteria, taskscould be categorized into four categories (as in following graph)</p><figure><img src="https://s2.loli.net/2022/02/17/SJzlLvZ1HBuCd3y.png"alt="The Eisenhower Matrix" /><figcaption aria-hidden="true">The Eisenhower Matrix</figcaption></figure><ol type="1"><li>Important &amp; Urgent tasks - should be done immediately and inperson.</li><li>Important &amp; Not Urgent tasks - should be done at certain datesand in person.</li><li>Unimportant &amp; Urgent tasks - could be delegated.</li><li>Unimportant &amp; Not Urgent tasks - should be dropped.</li></ol><p>In practice, it is not frequently listed but used as a mental modelto decide the task’s properties.</p><h3 id="pomodoros">Pomodoros</h3><p>This method was originally from Francesco Cirillo's <ahref="https://en.wikipedia.org/wiki/Pomodoro_Technique">PomodoroTechnique</a>. It took the name from a Pomodoro - tomato shaped kitchentimer. The “Pomodoro” is defined as the fundamental unit of time tomeasure tasks’ expected completion time, which is traditionally definedas being 30 minutes long, consisting of 25 minutes of work and 5 minutesof break time.</p><p>Though it may seems stiff to use strict pomodoros, it could actuallydevelop your sense of time and help you record your performance. Withcontinuous using of this technique, you could be more confident inpredicting tasks’ completion time. Recoding how many Pomodoros youfinished could also be a straight forward metric of evaluating yourperformance.</p><h3 id="gtd">GTD</h3><p>This strategy was created by <ahref="https://en.wikipedia.org/wiki/David_Allen_(author)">DavidAllen</a> in his book <ahref="https://en.wikipedia.org/wiki/Getting_Things_Done">Getting ThingsDone</a>. The basic idea of this strategy is to <strong>capture</strong>all unfinished things (projects or tasks), <strong>clarify</strong> theminto small actionable tasks with clear goals, organize<strong>them</strong> in appropriate forms (and come up to you at propertime), <strong>review</strong> tasks frequently to update the status and<strong>engage</strong> in these tasks.</p><p>In general, it provides a complete workflow for you to manage allyour tasks and projects in a personalized system. Though this system maybe hard to build and maintain, from my own experience, it is worthwhileto keep you confident in action decisions.</p><h2 id="my-implementation-of-time-management">My implementation of timemanagement</h2><p>To start with your time mangement, you frist need to be aware ofwhere you spent your time. I used <code>aTimeLogger</code> to track myeveryday activities. This is the start of my time management practiceand could provide a clear view of whether you are hard working orplaying too much. You could also pick your own time tracking app, justpay attention to the philosophy of the software to make sure they areNOT interruptive of your current activities and time-consuming whenrecording.</p><p><img src="https://s2.loli.net/2022/02/22/TtAapD1cR4I7v6g.jpg" alt="aTimeLogger Time Tracker· by BGCI" style="zoom:67%;" /></p><p>To plan your time carefully and wisely, I used GTD techniques totrack all the tasks I need to finish and all the events I need toparticipate. Following GTD’s principles, I would tag the tasks with itsexpected context (when would be proper, what tools needed, etc.) andgroup them with the assoicated project. I have tried plenty of taskmanagement apps like Omnifocus (which I am now using), Things 3,TickTick (Used for a long time), Wunderlist, etc. It is hard to give ageneral suggestion for the choice as different people may have differentrequirements, just make sure the app you choose helps record your taskseasily, intergrate nicely with your workflow and remind you inappropriate format.</p><p>One more thing to care about, is <strong>DO NOTovermanaging</strong>. I had once fallen into the pitfall ofovermanaging myself. I enjoyed listing and categorizing different tasksand projects, trying different apps, perfecting the tag system but justNOT complete many of those tasks. Remember what the name suggest,getting things done, so focus on completing the tasks rather thanputting most of your efforts changing their organization.</p><p>In the new information era, another thing has become more valuablethan our time, that is, our attention (or effective time). The power ofattention has long been discovered by physicist through <ahref="https://en.wikipedia.org/wiki/Double-slit_experiment">Double-slitexperiment</a>, in which whether observer exists could impact theresults of the inference pattern.</p><figure><img src="https://s2.loli.net/2022/02/18/xQ3H4IUFnDdfRoB.png"alt="Double-slit experiment" /><figcaption aria-hidden="true">Double-slit experiment</figcaption></figure><p>Modern business are even using our attention to gain money (throughonline advertisement). <strong>We need to pay attention to what we paidour attention to.</strong> What you should do, is paying attention tothe four things below:</p><ol type="1"><li>focusing on valuable things</li><li>relationships, especially intimacy</li><li>finding new trends</li><li>Self development</li></ol><p><strong>Pomodoros</strong> is therefore been introduced to my systemto record my attention (though it has way more potentials as introducedabove). You could easily track your progress and performance viapomodoro recoding. I am now using <code>Session</code> as the app torecord for its elegant visualizations and ease of use.</p>]]></content>
    
    
    <categories>
      
      <category>Informal Essays</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Methodology</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Thinking about writing</title>
    <link href="/2022/02/09/Thinking-about-writing/"/>
    <url>/2022/02/09/Thinking-about-writing/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this article, I will rethink the importance of writing, anddiscuss how the tools I used evolved.</p><span id="more"></span><h2 id="motivation">Motivation</h2><ul><li>In information era, the way we express ourselves matters.</li><li>Different thinking pattern could be reflected from the writing.</li><li>Among applications, we could use the most efficient way to expressand organize our ideas.</li></ul><h2 id="the-importance-of-writing">The importance of writing</h2><p>Writing, in its board aspect, is an everyday activity for all of us.However, according to my friends, little of us have ever thoughtcarefully about this activity. Different format we use could help ustrain different thinking patterns. Different applications we chose couldeffect the efficiency of expressing ourselves and organizing ideas. Itis also an important way for us to exchange serious opinions in a decentmanner (like publications).</p><p>Different format could be applied to writing. Within my knowledge,they are plain text (.txt, .word files), hypertext (.html, .md files)and outline (.opml files). Each format has an implication of differentthinking pattern. For example, if you use outlining more often thanother format, you are tend to be a person with a good sense of theoverall picture.If you use hypertext (e.g. markdown) very often, youtend to care about the content more than the format it is presented.</p><p>In Sönke Ahrens’s book <how to take smart notes>, he emphasis theimportance of writing:</p><blockquote><p>An idea kept private is as good as one you never had. And a fact noone can reproduce is no fact at all. <strong>Making something publicalways means to write it down</strong> so it can be read. There is nosuch thing as a history of unwritten ideas.</p></blockquote><h2 id="my-experience-in-writing">My experience in writing</h2><p>During my early life, I used Microsoft Word as my main editing toolas this is the only one I am familiar with. The software works fine,however, as I grow older, I tend to realize that I spend a lot of time<strong>editing the format</strong> for different headings andemphasis.</p><p>With my exploartion in computer science, I started to use markdown asthe main form of many writings. Markdown is a kind of mark up language,which uses a small set of symbols to realize the formatting. This freedme from doing manual adjustment for different parts of the text andcould now focus more on the content. Markdown is also a portable format.The formatting is realized by limited symbols which allows the raw textto be readable.In my university, I have been using markdown to takenotes, write reports and draft my dissertation.</p><p>Though markdown itself is a powerful concept, the editor could be ofgreat add on to this kind of writing. I have tried a lot of differentmarkdown editors and now kept mainly two of them (Typora and Obsidian),I will write more about the reasoning of choices I made in a laterpost.</p><p>As my writing in markdown format increases, I started to realize onedrawback for this format. It is more of a connstraint in its methodologyrather than software design. Markdown kinds of suggest a<strong>fragmented thinking</strong>, where each piece of markdown notestands for a concept or event solely. Though markdown provides linkfeature, it is not good for getting the whole picture (say you aretaking notes for a course and want to have an overview before exam).This let me to another kind of writing - outlining.</p><p>Compared to putting emphasis on the details, outline writing put anemphasis on the key points or the main topic. It encourages conciselanguage choices which is helpful for letting you know the biggerpicture and the key points. Using the outlining tools together with mymarkdown notes, I could both get an general understanding and havedetailed information when refering to my notes.</p><p>Later when I came accross the PKM concept, I began to use Obsidian tomanage my markdown notes, which could provide two-way linking betweendifferent files. How I implemented my own PKM system using Obsidiancould be found in another post.</p>]]></content>
    
    
    <categories>
      
      <category>Informal Essays</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Methodology</tag>
      
      <tag>Writing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>How to set up a hexo blog</title>
    <link href="/2021/11/28/How-to-set-up-a-hexo-blog/"/>
    <url>/2021/11/28/How-to-set-up-a-hexo-blog/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this article, you will find how to set up a hexo blog and deployit.</p><span id="more"></span><blockquote><p>Special thanks to <ahref="https://www.bilibili.com/video/BV1Yb411a7ty">Codesheep'svideo</a></p></blockquote><h1 id="pre-requisite">0. Pre-requisite</h1><p>The most important of all, don't be afraid of making mistakes!</p><p>To initiate your blog, you will need the libraries below.</p><h2 id="node.js">0.1 Node.js</h2><p><ahref="https://nodejs.org/en/download/"><code>Download here</code></a> -suggest LTS version</p><p>To check whether <code>Node.js</code> was downloaded successfully,type <code>node -v</code> in the terminal window. If something like<code>v12.18.2</code> is presented with no error message, you are freeto go for the next step.</p><blockquote><p>For Windows users, terminal could be invoked by pressing<code>win</code> + <code>R</code>, and then type <code>cmd</code> in theprompt window.</p></blockquote><h2 id="npm">0.2 npm</h2><p><code>npm</code> is the default package management tool for<code>Node.js</code>.</p><blockquote><p>For Chinese users, you could switch to the mirror in<code>cnpm</code> for faster downloads. Run the command to install<code>cnpm</code>: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs linux">$ npm install -g cnpm --registry=https://registry.npm.taobao.org<br></code></pre></td></tr></table></figure></p></blockquote><p>Similar to <code>Node.js</code>, type <code>npm -v</code> to check ifit's installed successfully.</p><h2 id="git">0.3 Git</h2><p><code>git</code> is a version control tool used widely amongprogramming projects. Generally speaking, it could help you keepdifferent versions of your project.</p><p><a href="https://git-scm.com/downloads">Download here</a></p><p>To configure the git globally, you could run the command afterinstallation. In the command, <code>--global</code> would set thefollowing parameters for all git repositories on your computer.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs linux">$ git config --global user.name &quot;Your Namee&quot;<br>$ git config --global user.email &quot;email@example.com&quot;<br></code></pre></td></tr></table></figure></p><h2 id="hexo">0.4 hexo</h2><p>With the package management tool installed, fetch the<code>hexo</code> package using command:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs linux">$ npm install -g hexo<br></code></pre></td></tr></table></figure><p>or: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs linux">$ cnpm install -g hexo<br></code></pre></td></tr></table></figure></p><p>Similar to <code>Node.js</code>, type <code>hexo -v</code> to checkif it's installed successfully.</p><h1 id="initiate-blog">1. Initiate blog</h1><ol type="1"><li><p>Create a folder</p><p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs linux">$ mkdir blog<br></code></pre></td></tr></table></figure></p><p>By default, this <code>blog/</code> folder will be created in<code>/Users/YourUserName/</code> for both Mac and Windowsusers.</p></li><li><p>Navigate into the folder</p><p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs linux">$ cd blog<br></code></pre></td></tr></table></figure></p><p>To check if you are in the right folder, type <code>pwd</code> tocheck.</p></li><li><p>Initiate your blog</p></li></ol><p><mark>Make sure you are in the right folder before running commandbelow!</mark></p><p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs linux">$ sudo hexo init<br></code></pre></td></tr></table></figure></p><p>The <code>sudo</code> parameter refers to admin access.</p><ol start="4" type="1"><li>Run your blog!</li></ol><p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs linux">$ hexo s<br></code></pre></td></tr></table></figure></p><p>By default, you will see the blog running athttp://localhost:4000</p><h1 id="create-a-new-article">2. Create a new article</h1><p>As indicated in the default blog file <code>helloworld.md</code>, youcould use the command below to create a new blog.</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs elixir"><span class="hljs-variable">$ </span>hexo n <span class="hljs-string">&quot;Your artical title&quot;</span><br></code></pre></td></tr></table></figure><p>By default, it will be created in <code>source/_posts</code> folder.For markdown reference, please check <ahref="https://commonmark.org/help/">here</a>.</p><p>After writing, use the following command to clean the database andgenerate static files for local blog.</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs verilog">$ hexo clean<br>$ hexo <span class="hljs-keyword">generate</span><br>$ hexo server<br></code></pre></td></tr></table></figure><p>Vist the default link again, you could see the article you justwrote.</p><h1 id="deploy-your-blog">3. Deploy your blog</h1><blockquote><p>You will need a Github account or Gitee accound for deployment.</p></blockquote><h2 id="github">Github</h2><ol type="1"><li><p>Create a new repository with the name<code>YourFullUserName.github.io</code> and it has to be set topublic.</p></li><li><p>Install a plugin for deployment.</p><p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">$ npm install <span class="hljs-comment">--save hexo-deployer-git</span><br></code></pre></td></tr></table></figure></p></li><li><p>Find the <code>_config.yml</code> file in your blog folder, atthe end of this file, modify the following content:</p><p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-comment"># Deployment</span><br><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="hljs-attribute">deploy</span><span class="hljs-punctuation">:</span><br>  <span class="hljs-attribute">type</span><span class="hljs-punctuation">:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attribute">repo</span><span class="hljs-punctuation">:</span> <span class="hljs-string">Your Github repo&#x27;s link</span><br>  <span class="hljs-attribute">branch</span><span class="hljs-punctuation">:</span> <span class="hljs-string">master</span><br></code></pre></td></tr></table></figure></p><p>Your repo link should end with <code>.github.io.git</code> which youcould obtain here.</p><figure><img src="https://i.loli.net/2021/11/28/mQEldKOZv9wsjqc.jpg"alt="Where to find your link" /><figcaption aria-hidden="true">Where to find your link</figcaption></figure></li><li><p>After saving your config file, run command below to deploy yourblog.</p><p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>hexo d<br></code></pre></td></tr></table></figure></p><p>You may need to type your user name and password.</p></li><li><p>See you blog online!</p><p>Visit <code>yourUserName.github.io</code></p></li></ol><h2 id="gitee">Gitee</h2><p>#TODO</p><h1 id="references">References</h1><ul><li><a href="https://www.bilibili.com/video/BV1Yb411a7ty">Codesheep'svideo in Chinese</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>translation</tag>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Index System Introduction</title>
    <link href="/2021/09/27/Index%20System%20Introduction/"/>
    <url>/2021/09/27/Index%20System%20Introduction/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this article, you will find an brief definition of Index system,how is it useful and how to build one on your own.</p><span id="more"></span><p>Further to my internship experience and my reading on the web, I haveconcluded some information about index system.</p><h2 id="what-is-index-system">1. What is Index System?</h2><p>For example, when deciding whether a company is worth investing, youwould see comments like "This company has too much loans", "It is notmaking any profits". However in data-driven industries (like finance),you need to support your decisions with intuitive evidence. Instead ofraw business data, an index should be used for demonstration purposes(e.g. Asset-liability ratio).In the above example, the index ofasset-liability ration addresses the company's debt.</p><p>In real world problem, complicate problem could not be resolved byusing one single index (e.g. Evaluating a company's futureprofitability), therefore a set of indexes is needed to describe thedata subject from different perspectives and lead to a informeddecision, that is where index system plays a role.</p><h2 id="how-is-index-system-useful">2. How is index system useful?</h2><p>When evaluating one's health condition, we tend to use the indicatorslike body fat percentage, body temperature, blood pressure, etc.Considering these indicators together, one's health status could then bedetermined.</p><p>It's the same when using index system to evaluate a company. Whensomething goes wrong, the index system should be able to reflect thisabnormality. Tracing down the problematic index that goes wrong, thecurrent problem in business should be clear. And then the relevantdepartment could have the right direction to improve on.</p><p>In summary, the index system should achieve the following: 1. Monitorbusiness situation 2. Find problems based on problematic index 3.Evaluate business and guide the future work.</p><h2 id="how-to-build-an-index-system">3. How to build an indexsystem?</h2><p>The general approach is as following: 1. Understand company's /department's Key Performance Indicator (KPI), define Level-1 Index 2.Understand the business, dismantle Level-1 Index and define Level-2Index 3. Sort out business processes, dismantle Level-2 Index and defineLevel-3 Index 4. Using reports to monitor index system, update itaccordingly</p><h2 id="common-mistakes-when-building-index-system">4. Common mistakeswhen building index system</h2><ul><li>Not understand the KPI</li><li>No logic relationship between indexes</li><li>Dissembled indexes has no business meaning</li><li>Little or No communication between data department and businessdepartment</li></ul><h2 id="reference">Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/285551902">Zhihu's passage -How to build an index system by Houzi</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Data Science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>translation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Binary search algorithm</title>
    <link href="/2021/09/11/Binary%20search%20algorithm%20Summary/"/>
    <url>/2021/09/11/Binary%20search%20algorithm%20Summary/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this post, I will give a brief summary of binary search algorithm,together with some programming problems on this topic.</p><span id="more"></span><h2 id="algorithm-introduction">Algorithm Introduction</h2><ul><li>It is a simple <strong>recursive</strong> searching algorithm.</li><li>Scope of application: in <strong>ordered</strong> arrays</li></ul><p>The pseudocode<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> is asfollows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">Procedure binary_search<br>   A ← <span class="hljs-built_in">sorted</span> array<br>   n ← size of array<br>   x ← value to be searched<br>   <span class="hljs-type">Set</span> lowerBound = <span class="hljs-number">1</span> <br>   <span class="hljs-type">Set</span> upperBound = n <br>   <span class="hljs-keyword">while</span> x <span class="hljs-keyword">not</span> found <br>    <span class="hljs-keyword">if</span> upperBound &lt; lowerBound <br>EXIT: x does <span class="hljs-keyword">not</span> exists. <br><span class="hljs-built_in">set</span> midPoint = lowerBound + ( upperBound - lowerBound ) / <span class="hljs-number">2</span> <br> <br> <span class="hljs-keyword">if</span> A[midPoint] &lt; x <br> <span class="hljs-built_in">set</span> lowerBound = midPoint + <span class="hljs-number">1</span> <br> <span class="hljs-keyword">if</span> A[midPoint] &gt; x <br> <span class="hljs-built_in">set</span> upperBound = midPoint - <span class="hljs-number">1</span> <br> <span class="hljs-keyword">if</span> A[midPoint] = x <br>         EXIT: x found at location midPoint <br>   end <span class="hljs-keyword">while</span><br></code></pre></td></tr></table></figure><p>In plain words, it recursively check whether the mid point value isthe target, if the target is bigger, check the left half; if the targetis smaller, check the right half.</p><h3 id="complexity">Complexity</h3><ul><li>Space complexity Only constant number of variables are used,therefore the space complexity is <spanclass="math inline">\(O(1)\)</span>.</li><li>Time complexity Because each time after search, half of the arraywould be 'discarded', therefore the overall complexity is <spanclass="math inline">\(O(\log n)\)</span>, where <spanclass="math inline">\(n\)</span> is the length of the array.</li></ul><h2 id="java-implementation-example">Java Implementation example</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// Java - Non-recursive version</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>&#123;<br><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">binarySearch</span><span class="hljs-params">(<span class="hljs-type">int</span>[] nums, <span class="hljs-type">int</span> target)</span> &#123;<br><span class="hljs-type">int</span> <span class="hljs-variable">n</span> <span class="hljs-operator">=</span> nums.length;<br><span class="hljs-type">int</span> <span class="hljs-variable">left</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>, right = n - <span class="hljs-number">1</span>;<br><span class="hljs-keyword">while</span> (left &lt;= right) &#123;<br><span class="hljs-type">int</span> <span class="hljs-variable">mid</span> <span class="hljs-operator">=</span> left + (right - left) / <span class="hljs-number">2</span>; <span class="hljs-comment">// Prevent add overflow</span><br><span class="hljs-keyword">if</span> (nums[mid] == target) <span class="hljs-keyword">return</span> mid;<br><span class="hljs-keyword">if</span> (nums[mid] &lt; target) &#123;<br>left = mid + <span class="hljs-number">1</span>;<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>right = mid - <span class="hljs-number">1</span>;<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;<br>&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// Java - Revursive version</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>&#123;<br><span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">binarySearch</span><span class="hljs-params">(<span class="hljs-type">int</span>[] nums, <span class="hljs-type">int</span> start, <span class="hljs-type">int</span> end, <span class="hljs-type">int</span> target)</span> &#123;<br><span class="hljs-keyword">if</span> (start &gt; end)<br><span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>;<br>    <span class="hljs-type">int</span> <span class="hljs-variable">mid</span> <span class="hljs-operator">=</span> start + (end - start)/<span class="hljs-number">2</span>; <span class="hljs-comment">// Prevent add overflow</span><br>    <span class="hljs-keyword">if</span> (arr[mid] &gt; target)<br>        <span class="hljs-keyword">return</span> binarySearch(arr, start, mid - <span class="hljs-number">1</span>, target);<br>    <span class="hljs-keyword">if</span> (arr[mid] &lt; target)<br>        <span class="hljs-keyword">return</span> binarySearch(arr, mid + <span class="hljs-number">1</span>, end, target);<br>    <span class="hljs-keyword">return</span> mid;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="questions">Questions</h2><ol type="1"><li><a href="https://leetcode.com/problems/binary-search/">Leetcode 704- Binary search</a> - Basic implementation for this algorithm.</li><li><a href="https://leetcode.com/problems/first-bad-version/">Leetcode278 - First bad Version</a> - Variation on look-up standards.</li><li><ahref="https://leetcode.com/problems/find-first-and-last-position-of-element-in-sorted-array/">Leetcode34 - Find First and Last Position of element in Sorted array</a> -Variation on look-up standards and re-use of code.</li><li><ahref="https://leetcode.com/problems/search-in-rotated-sorted-array/">Leetcode33 - Search in rotated sorted array</a>- Variation on the structure ofarray.</li><li><a href="https://leetcode.com/problems/search-a-2d-matrix/">Leetcode74 - Search a 2D matrix</a> - Variation on the structure of input array,need to reduce the dimension of input.</li><li><a href="https://leetcode.com/problems/find-peak-element/">Leetcode162 - Find peak element</a> - Another realization of binary searchalgorithm.</li></ol><div id="footnotes"><hr><div id="footnotelist"><ol><li id="fn:1">The pseudo code is derived from<a href="https://www.tutorialspoint.com/data_structures_algorithms/binary_search_algorithm.htm">tutorialspoint</a><a href="#fnref:1" rev="footnote"> ↩︎</a></li></ol></div></div>]]></content>
    
    
    <categories>
      
      <category>Computer Science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Intention of this blog</title>
    <link href="/2021/08/18/Intention-of-this-blog/"/>
    <url>/2021/08/18/Intention-of-this-blog/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span><h2 id="aims">Aims</h2><ul><li>Record feelings and findings during CS study.</li><li>Share my project experience with reproduceble steps.</li><li>Publish notes about courses and libraies.</li></ul><h2 id="why-this-blog-is-named-connecting-dots">Why this blog is namedconnecting dots?</h2><p>In childhood, I was fond of the game <ahref="https://en.wikipedia.org/wiki/Connect_the_dots">Connect thedots</a>. It seems magic to me that drawing lines between numbered dotsgradually reveals the actual shape hidden behind. Sometimes it's toughto predict what you will arrive at simply by looking at the outline ofthe unconnected dots and therefore finishing the game always brings mesurprise and excitement.</p><p>As I grew older, I thought about the ultimate philosophicalquestions: Who am I? Where am I heading? Reading lots of biographies didnot offer me a satisfactory answer, as I found that most of the time,people aren't quite sure where they will be taken to by theiractions.</p><p>It struck me one day that life is just like the game I enjoyed in mychildhood. Finishing school, completing projects, and entering arelationship are all actions that create a 'dot' on the canvas of ourlife. It is not until some point in life when we look back, that we seea clear picture of our past by connecting the 'dots' we created.</p><p>Therefore, I named my blog after my favourite childhood game torecord the dots I made in my life and remind me to constantly reviewwhere these dots may take me.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Basic Blog Operations</title>
    <link href="/2021/08/17/hello-world/"/>
    <url>/2021/08/17/hello-world/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Operating System Lecture Notes - Introduction</title>
    <link href="/2020/10/04/OSC%20Lecture%20Notes%20-%20Introduction/"/>
    <url>/2020/10/04/OSC%20Lecture%20Notes%20-%20Introduction/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In this note, you could find the basic definition of OperatingSystems and the general architecture of computer.</p><span id="more"></span><h2 id="defining-operating-systems">Defining Operating Systems</h2><h3 id="what-can-an-os-do-for-me">What can an OS do for me?</h3><ul><li><p><strong>File systems</strong>: where is the file physicallywritten on the disk and how is it retrieved?</p></li><li><p><strong>Abstraction</strong>: why looks the instruction the sameindependent of the device?</p></li><li><p><strong>Concurrency</strong>: what if multiple programs accessthe same file simultaneously? What if an other process startsrunning?</p></li><li><p><strong>Security</strong>: why is the access denied? Where inmemory will the array be stored and how is it protected fromunauthorised access?</p></li><li><p>What if the array requires more memory than physicallyavailable?</p></li><li><p>What if only part of the array is currently in use ?</p></li></ul><h4 id="what-is-part-of-the-operating-system">What is part of theoperating system?</h4><p>Memory management, CPU scheduling, file system, communication, memorymanagement, interrupt handling, GUI, . . .</p><h4 id="a-resource-manager">A resource manager</h4><ul><li>Many modern operating systems use <strong>multi-programming</strong>to <strong>improve user experience</strong> and <strong>maximizeresource utilization</strong><ul><li>Disks are slow: without multi-programming, CPU time is wasted whilewaiting for I/O requests<ul><li>Imagine a CPU running at 3.2 GHz (approx. 3:2 <spanclass="math inline">\(\times\)</span> 109 instructions per second)</li><li>Imagine a disk rotating at 7200 RPM, taking 4.2 ms to rotate half atrack</li><li>I/O is slow, we are missing out on 3.2 <spanclass="math inline">\(\times\)</span> 4.2 <spanclass="math inline">\(\times\)</span> <spanclass="math inline">\(10^6\)</span> instructions (13.44m)!</li></ul></li></ul></li><li>The implementation of multi-programming has important consequencesfor operating system design</li><li>The operating system must <strong>allocate/share resources</strong>(including CPU, memory, I/O devices) fairly and safely between competingprocesses:<ul><li>In time, e.g. CPUs and printers</li><li>In space, e.g., memory and disks</li></ul></li><li>The execution of multiple programs (processes) needs to beinterleaved with one another:<ul><li>This requires context switches and process scheduling ) <spanclass="math inline">\(\Rightarrow\)</span> mutual exclusion, deadlockavoidance, protection, . .</li></ul></li></ul><h3 id="origin">Origin</h3><p>In the early days, programmers had to deal directly with thehardware</p><ul><li>Real computer hardware is ugly</li><li>Hardware is extremely difficult to manipulate/program</li></ul><p><mark>An operating system is a layer of indirection on top of thehardware</mark>:</p><ul><li>It provide <strong>abstractions</strong> for application programs(e.g., file systems)</li><li>It provides a <strong>cleaner and easier interface to thehardware</strong> and hides the complexity of “bare metal”</li><li>It allows the programmer to be lazy by using common routines:-)</li></ul><p><img src="https://s2.loli.net/2022/01/09/Fi8pODhsMjUYQWe.png" alt="image-20201004095954244" style="zoom:67%;" /></p><h3 id="why-study-operating-system">Why study operating system?</h3><ul><li>The programs that we write <strong>use operating systemfunctionality</strong></li><li>How are the operating system’s services/abstractionsimplemented</li></ul><h2 id="computer-architecture">Computer Architecture</h2><p><img src="https://s2.loli.net/2022/01/09/6zOLWYRhwxfPS48.png" alt="image-20201004100314213" style="zoom:50%;" /></p><center>Simplified computer model (Tanenbaum, 2014)</center><h3 id="cpu-design">CPU design</h3><ul><li>CPU’s basic cycle consist of <strong>fetch, decode, andexecute</strong> (pipelines, or superscalar)</li><li>Every CPU has his own instruction set</li><li>A CPU has a set of registers (extremely fast memory close to the CPU“core”)<ul><li>Registers are used to <strong>store data and for specialfunctions</strong> (e.g. program counter, program status word – modebit)</li><li>The <strong>compiler/programmer</strong> decides what to keep in theregisters</li><li><strong>Context switching</strong> must save and restore the CPU’sinternal state, including its registers</li></ul></li></ul><figure><img src="https://s2.loli.net/2022/01/09/BXFD78eJvCzyScn.png"alt="image-20201004100845955" /><figcaption aria-hidden="true">image-20201004100845955</figcaption></figure><h4 id="memory-management-unit">Memory management Unit</h4><ul><li><p>There are two different address spaces:</p><ul><li>the <strong>logical address space</strong> seen by the process andused by the compiler</li><li>the <strong>physical address space</strong> seen by thehardware/OS</li></ul></li><li><p>When compiling code, memory addresses must be assigned tovariables and instructions, the compiler does not know what memoryaddresses will be available in physical memory</p></li><li><p>It will just assume that the code will start running at address 0when generating the machine code</p><p><img src="https://s2.loli.net/2022/01/09/Ug1EDLKpfQdIZqV.png" alt="image-20201004102110699" style="zoom:67%;" /></p></li><li><p>On some rare occasions, the process may run at physical address0</p><ul><li>physical address = logical address + 0</li></ul></li><li><p>On other occasions, it will be running at a completely differentlocation in physical memory and an offset is added</p><ul><li>physical address = logical address + offset</li></ul></li><li><p>The <code>memory management unit</code>(MMU) is<strong>responsible for address translation</strong> (“adding theoffset”)</p><ul><li>Different processes require different address translation(offsets)</li><li>Context switching requires the MMU to be updated (and registers,cache, ...)</li></ul></li></ul><h5 id="example">Example</h5><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-type">int</span> iVar = <span class="hljs-number">0</span>;<br><span class="hljs-type">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span>(i &lt; <span class="hljs-number">10</span>) &#123;<br>        iVar++;<br>        sleep(<span class="hljs-number">2</span>);<br>        <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Address:%u; Value:%d\n&quot;</span>,&amp;iVar, iVar);<br>         i++;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>The same addresses will be displayed for <code>iVar</code>. Theaddress printed on the screen is the logical address</li><li>The value for <code>iVar</code> in the first run doesn’t influencethe second run’s value</li></ul><h3 id="moores-law">Moore’s Law</h3><blockquote><p>“The number of transistors on an integrated circuit (chip) doublesroughly every two years”</p></blockquote><ul><li>Closely linked, but not necessarily related to performance</li><li>Moore’s still continuing, but the “power wall” slows performanceimprovements of <strong>single core/single processor systems</strong><ul><li>A few cores for multiple “programs” is easy to justify</li><li>How to use <strong>massively parallel</strong> computers/CPUs/manycore machines</li><li>Can we extract parallelism automatically, can we implementparallelism at the lowest level (similar to multiprogramming)</li></ul></li></ul><blockquote><p>Lead to multi-core / parallel development</p></blockquote><h3 id="multi-core-hyperthreaded-processors">Multi-core, hyperthreadedprocessors</h3><ul><li><p>Modern CPUs contain multiple cores and are oftenhyper-threaded</p></li><li><p><strong>Evolution in hardware</strong> has implications on<strong>operating system design</strong></p><ul><li><p>XP did not support <strong>multi processorarchitectures</strong></p></li><li><p><strong>Process scheduling</strong> needs to account for loadbalancing and CPU affinity</p></li><li><p>Cache <strong>coherency</strong> becomes important (managerun-time data)</p><p><img src="https://s2.loli.net/2022/01/09/nlVhBsdvXeZO7uJ.png" alt="image-20201004111125457" style="zoom:50%;" /></p></li></ul></li></ul><blockquote><p>Previous exam: Describe how, in your opinion, recent developments incomputer architecture and computer design have influenced operatingsystem design</p></blockquote><h3 id="timer-interrupts">Timer Interrupts</h3><ul><li>Interrupts temporarily pause a process’s normal operation</li><li>Different types of interrupts exist, including:<ul><li>Timer interrupts by <strong>CPU clock</strong></li><li><strong>I/O interrupts</strong> for <strong>I/O completion</strong>or error codes</li><li><strong>Software generated</strong>, e.g. errors and exceptions</li></ul></li><li><strong>Context switches</strong> (i.e. switching between processes)can be initiated by timer interrupts after a “set time”</li></ul><p><img src="https://s2.loli.net/2022/01/09/lHoSLPmFs6bNGny.png" alt="image-20201004111301287" style="zoom:50%;" /></p><ol type="1"><li>Timer generates an interrupt</li><li>CPU finishes current instruction and tests for interrupt</li><li>Transfer to interrupt service routine<ul><li>Hardware saves current process state (PSW, program counter)</li><li>Set program counter to interrupt service routine</li><li>Save registers and other state information</li></ul></li><li>Carry out interrupt service routine (scheduler)</li><li>Restore next process to run</li></ol>]]></content>
    
    
    <categories>
      
      <category>Computer Science</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Note</tag>
      
      <tag>Operating System</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
